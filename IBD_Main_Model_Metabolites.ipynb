{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###Import necessary libraries"
      ],
      "metadata": {
        "id": "_H6W9nPiziog"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "7RutIbaK1fyT"
      },
      "outputs": [],
      "source": [
        "! pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TD-V3kRPTdU1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import os,os.path\n",
        "import re\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "import optuna\n",
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.utils import resample\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from scipy.stats import bootstrap"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Load the datasets"
      ],
      "metadata": {
        "id": "Om5Tl0oGetyv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KN4C5pkGUzMu"
      },
      "outputs": [],
      "source": [
        "mtb = pd.read_csv('mtb_scaled_ibd.csv')\n",
        "mtb"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#XGBOOST"
      ],
      "metadata": {
        "id": "b0iv9_LIcO6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = mtb.drop(['Study.Group'], axis=1)\n",
        "y = mtb['Study.Group']\n",
        "\n",
        "# Encode categorical target labels into numerical labels\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Initialize XGBoost classifier\n",
        "model = xgb.XGBClassifier(eval_metric='logloss', random_state=42)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the original test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "print(f'Test Precision: {precision:.4f}')\n",
        "print(f'Test F1 Score: {f1:.4f}')\n",
        "print(f'Test Recall: {recall:.4f}')\n"
      ],
      "metadata": {
        "id": "KsocN74fbw75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Random Search"
      ],
      "metadata": {
        "id": "AMMNXpnpUPP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "param_dist = {\n",
        "    'n_estimators': np.arange(100, 500, 50),\n",
        "    'max_depth': np.arange(4, 10),\n",
        "    'learning_rate': np.linspace(0.01, 0.2, 10),\n",
        "    'subsample': np.linspace(0.5, 1.0, 10),\n",
        "    'colsample_bytree': np.linspace(0.5, 1.0, 10),\n",
        "    'gamma': np.linspace(0, 0.5, 5),\n",
        "    'min_child_weight': np.arange(1, 6),\n",
        "    'alpha': np.logspace(-3, 1, 5),\n",
        "    'lambda': np.logspace(-3, 1, 5)\n",
        "}\n",
        "\n",
        "\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=XGBClassifier(random_state=42),\n",
        "    param_distributions=param_dist, n_iter=200,\n",
        "    scoring='roc_auc', cv=5, verbose=1, random_state=42, n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "best_params = random_search.best_params_\n",
        "\n",
        "# Initialize the XGBoost classifier with early stopping\n",
        "xgb_best = XGBClassifier(**best_params, random_state=42,\n",
        "                         early_stopping_rounds=10,\n",
        "                         eval_metric='logloss')\n",
        "\n",
        "# Fit the model on the full training set\n",
        "xgb_best.fit(\n",
        "    X_train, y_train,\n",
        "    eval_set=[(X_test, y_test)],\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = xgb_best.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Print the metrics\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1 Score: {f1:.2f}\")"
      ],
      "metadata": {
        "id": "ODpNyIZPcOVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Best Parameters: {random_search.best_params_}\")\n",
        "print(f\"Best Cross-validation Accuracy: {random_search.best_score_:.4f}\")"
      ],
      "metadata": {
        "id": "S4kVjrxHorUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bayesian Optimization"
      ],
      "metadata": {
        "id": "P3XyDsJZU0Ai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial):\n",
        "    params = {\n",
        "        'booster': trial.suggest_categorical('booster', ['gbtree', 'dart']),\n",
        "        'objective': 'binary:logistic',\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
        "        'gamma': trial.suggest_float('gamma', 0, 0.5),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
        "        'min_child_weight': trial.suggest_float('min_child_weight', 0.5, 5),\n",
        "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
        "        'lambda': trial.suggest_float('lambda', 1e-8, 1.0, log=True),\n",
        "        'alpha': trial.suggest_float('alpha', 1e-8, 1.0, log=True),\n",
        "        # Adjusting the range to include 400 as the maximum\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 50, 400)\n",
        "    }\n",
        "\n",
        "    # Initialize the XGBoost model with the suggested hyperparameters\n",
        "    model = xgb.XGBClassifier(**params, eval_metric='logloss')\n",
        "\n",
        "    # Use StratifiedKFold to maintain the class distribution\n",
        "    skf = StratifiedKFold(n_splits=5)\n",
        "\n",
        "    # Evaluate using cross-validation on the data\n",
        "    cv_scores = cross_val_score(model, X_train, y_train, cv=skf, scoring='roc_auc')\n",
        "\n",
        "    # Return the mean AUC-ROC from cross-validation\n",
        "    return cv_scores.mean()\n",
        "\n",
        "# Create a study to maximize AUC-ROC\n",
        "study = optuna.create_study(direction='maximize')\n",
        "\n",
        "# Enqueue the parameters obtained from previous RandomizedSearchCV results\n",
        "study.enqueue_trial({\n",
        "    'booster': 'gbtree',\n",
        "    'objective': 'binary:logistic',\n",
        "    'learning_rate': 0.2,\n",
        "    'gamma':  0.0,\n",
        "    'max_depth': 8,\n",
        "    'min_child_weight': 1,\n",
        "    'subsample': 0.6111111111111112,\n",
        "    'colsample_bytree': 0.7777777777777778,\n",
        "    'n_estimators': 400,\n",
        "    'lambda': 0.01,\n",
        "    'alpha': 0.001\n",
        "})\n",
        "\n",
        "# Optimize the study using 50 trials\n",
        "study.optimize(objective, n_trials=50)\n",
        "\n",
        "# Print the best parameters and cross-validation AUC-ROC\n",
        "print(f\"Best Parameters: {study.best_params}\")\n",
        "print(f\"Best Cross-validation AUC-ROC: {study.best_value:.4f}\")\n",
        "\n",
        "# Train the final model with the best parameters on the training data\n",
        "best_params = study.best_params\n",
        "final_model = xgb.XGBClassifier(**best_params, eval_metric='logloss')\n",
        "final_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the original test set\n",
        "y_pred = final_model.predict(X_test)\n",
        "y_pred_proba = final_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluate the final model on the test set\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "# Print test set performance metrics\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Test Precision: {precision:.4f}\")\n",
        "print(f\"Test F1 Score: {f1:.4f}\")\n",
        "print(f\"Test Recall: {recall:.4f}\")\n",
        "print(f\"Test AUC-ROC: {roc_auc:.4f}\")\n"
      ],
      "metadata": {
        "id": "pOA-zEFYcu1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Best parameters from Bayesian Optimization XGBoost\n",
        "best_params_xg = {\n",
        "    'learning_rate': 0.17008256693104395,\n",
        "    'max_depth': 15,\n",
        "    'n_estimators': 178,\n",
        "    'gamma': 0.0030531517878193103,\n",
        "    'min_child_weight': 0.5024232638699605,\n",
        "    'subsample': 0.5288178444987158,\n",
        "    'colsample_bytree': 0.9423605692686023,\n",
        "    'objective': 'binary:logistic',\n",
        "    'booster': 'gbtree',\n",
        "    'lambda': 2.5159211654207813e-06,\n",
        "    'alpha': 3.018073013444745e-07\n",
        "\n",
        "}\n",
        "\n",
        "# Create the XGBoost classifier with the best parameters\n",
        "final_model_xg = xgb.XGBClassifier(**best_params_xg)\n",
        "\n",
        "# Perform cross-validation using the training data\n",
        "cv_scores = cross_val_score(final_model_xg, X_train, y_train, cv=5, scoring='roc_auc')\n",
        "print(\"Cross-validation scores:\")\n",
        "print(cv_scores)\n",
        "print(f\"Mean CV accuracy: {np.mean(cv_scores):.4f}\")\n",
        "\n",
        "# Train the model using the training data\n",
        "final_model_xg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the original test set\n",
        "y_pred = final_model_xg.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics on the test set\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print the results\n",
        "print(f'Test Accuracy: {accuracy:.4f}')\n",
        "print(f'Test Precision: {precision:.4f}')\n",
        "print(f'Test Recall: {recall:.4f}')\n",
        "print(f'Test F1-Score: {f1:.4f}')\n"
      ],
      "metadata": {
        "id": "MYifRFyHRvDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Feature Selection For XGBOOST(RFECV)"
      ],
      "metadata": {
        "id": "bmaPVf0vcrHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_xg, X_test_xg, y_train_xg, y_test_xg = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "final_model_xg = xgb.XGBClassifier(**best_params_xg)\n",
        "\n",
        "# Define RFECV\n",
        "selector = RFECV(estimator=final_model_xg, step=20, cv=StratifiedKFold(10), scoring='roc_auc', verbose=2)\n",
        "\n",
        "# Fit the RFECV selector on the training data\n",
        "selector.fit(X_train_xg, y_train_xg)\n",
        "\n",
        "# Print the optimal number of features\n",
        "print(f\"Optimal number of features: {selector.n_features_}\")\n",
        "\n",
        "# Get the names of the selected features\n",
        "selected_features_mask = selector.support_\n",
        "selected_features_xg = X.columns[selected_features_mask]\n",
        "\n",
        "# Print the selected features\n",
        "print(\"Selected features:\", selected_features_xg)\n",
        "\n",
        "# Train the final model with selected features on the training set\n",
        "final_model_xg.fit(X_train_xg[selected_features_xg], y_train_xg)\n",
        "\n",
        "# Make predictions on the test set using only the selected features\n",
        "y_pred = final_model_xg.predict(X_test_xg[selected_features_xg])\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test_xg, y_pred)\n",
        "precision = precision_score(y_test_xg, y_pred)\n",
        "recall = recall_score(y_test_xg, y_pred)\n",
        "f1 = f1_score(y_test_xg, y_pred)\n",
        "\n",
        "# Print the results\n",
        "print(f'Test Accuracy: {accuracy:.4f}')\n",
        "print(f'Test Precision: {precision:.4f}')\n",
        "print(f'Test Recall: {recall:.4f}')\n",
        "print(f'Test F1-Score: {f1:.4f}')"
      ],
      "metadata": {
        "id": "x198gRSa0QoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_selected_xg = selector.transform(X_train_xg)\n",
        "X_test_selected_xg = selector.transform(X_test)\n",
        "\n",
        "# Train the final model with the selected features on the training set\n",
        "final_model_xg.fit(X_train_selected_xg, y_train_xg)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_prob_xg = final_model_xg.predict_proba(X_test_selected_xg)[:, 1]\n",
        "y_pred_xg = final_model_xg.predict(X_test_selected_xg)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test_xg, y_pred_xg)\n",
        "precision = precision_score(y_test_xg, y_pred_xg)\n",
        "recall = recall_score(y_test_xg, y_pred_xg)\n",
        "f1 = f1_score(y_test_xg, y_pred_xg)\n",
        "roc_auc = roc_auc_score(y_test_xg, y_pred_prob_xg)\n",
        "\n",
        "# Calculate specificity\n",
        "tn, fp, fn, tp = confusion_matrix(y_test_xg, y_pred_xg).ravel()\n",
        "specificity = tn / (tn + fp)\n",
        "\n",
        "# Print the results\n",
        "print(f'Test ROC AUC: {roc_auc:.2f}')\n",
        "print(f'Test Accuracy: {accuracy:.2f}')\n",
        "print(f'Test Precision: {precision:.2f}')\n",
        "print(f'Test Recall: {recall:.2f}')\n",
        "print(f'Test F1-Score: {f1:.2f}')\n",
        "print(f'Test Specificity: {specificity:.2f}')"
      ],
      "metadata": {
        "id": "mb0BomRREdvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##95% CI"
      ],
      "metadata": {
        "id": "kCgESUbAU4ok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate evaluation metrics\n",
        "def calculate_metrics(y_test, y_pred, y_pred_prob):\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
        "\n",
        "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
        "    specificity = tn / (tn + fp)\n",
        "\n",
        "    return accuracy, precision, recall, f1, roc_auc, specificity\n",
        "\n",
        "# Function to compute 95% confidence intervals using bootstrapping\n",
        "def bootstrap_ci(y_test, y_pred, y_pred_prob, n_bootstraps=1000, ci=95):\n",
        "    bootstrapped_scores = {\n",
        "        'accuracy': [],\n",
        "        'precision': [],\n",
        "        'recall': [],\n",
        "        'f1': [],\n",
        "        'roc_auc': [],\n",
        "        'specificity': []\n",
        "    }\n",
        "\n",
        "    for i in range(n_bootstraps):\n",
        "        # Resample the test data\n",
        "        indices = resample(np.arange(len(y_test)), random_state=i)\n",
        "        y_test_resampled = y_test[indices]\n",
        "        y_pred_resampled = y_pred[indices]\n",
        "        y_pred_prob_resampled = y_pred_prob[indices]\n",
        "\n",
        "        # Calculate metrics for the resampled data\n",
        "        accuracy, precision, recall, f1, roc_auc, specificity = calculate_metrics(\n",
        "            y_test_resampled, y_pred_resampled, y_pred_prob_resampled\n",
        "        )\n",
        "\n",
        "        # Store the scores\n",
        "        bootstrapped_scores['accuracy'].append(accuracy)\n",
        "        bootstrapped_scores['precision'].append(precision)\n",
        "        bootstrapped_scores['recall'].append(recall)\n",
        "        bootstrapped_scores['f1'].append(f1)\n",
        "        bootstrapped_scores['roc_auc'].append(roc_auc)\n",
        "        bootstrapped_scores['specificity'].append(specificity)\n",
        "\n",
        "    # Calculate percentiles for confidence intervals\n",
        "    ci_lower = (100 - ci) / 2\n",
        "    ci_upper = 100 - ci_lower\n",
        "\n",
        "    metrics_ci = {\n",
        "        metric: (np.percentile(scores, ci_lower), np.percentile(scores, ci_upper))\n",
        "        for metric, scores in bootstrapped_scores.items()\n",
        "    }\n",
        "\n",
        "    return metrics_ci\n",
        "\n",
        "# After training your model and getting predictions\n",
        "accuracy, precision, recall, f1, roc_auc, specificity = calculate_metrics(y_test, y_pred_xg, y_pred_prob_xg)\n",
        "\n",
        "# Compute the 95% confidence intervals\n",
        "metrics_ci = bootstrap_ci(np.array(y_test), np.array(y_pred_xg), np.array(y_pred_prob_xg))\n",
        "\n",
        "# Print the metrics and their confidence intervals\n",
        "print(f'Test ROC AUC: {roc_auc:.2f} (95% CI: {metrics_ci[\"roc_auc\"][0]:.2f}, {metrics_ci[\"roc_auc\"][1]:.2f})')\n",
        "print(f'Test Accuracy: {accuracy:.2f} (95% CI: {metrics_ci[\"accuracy\"][0]:.2f}, {metrics_ci[\"accuracy\"][1]:.2f})')\n",
        "print(f'Test Precision: {precision:.2f} (95% CI: {metrics_ci[\"precision\"][0]:.2f}, {metrics_ci[\"precision\"][1]:.2f})')\n",
        "print(f'Test Recall: {recall:.2f} (95% CI: {metrics_ci[\"recall\"][0]:.2f}, {metrics_ci[\"recall\"][1]:.2f})')\n",
        "print(f'Test F1-Score: {f1:.2f} (95% CI: {metrics_ci[\"f1\"][0]:.2f}, {metrics_ci[\"f1\"][1]:.2f})')\n",
        "print(f'Test Specificity: {specificity:.2f} (95% CI: {metrics_ci[\"specificity\"][0]:.2f}, {metrics_ci[\"specificity\"][1]:.2f})')\n"
      ],
      "metadata": {
        "id": "toW01fXm1gfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RANDOM FOREST"
      ],
      "metadata": {
        "id": "Ljm4PpZughFA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = mtb.drop(['Study.Group'], axis=1)\n",
        "y = mtb['Study.Group']\n",
        "\n",
        "# Encode categorical target labels into numerical labels\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Create the Random Forest classifier\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the classifier on the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy on the test set\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n",
        "\n",
        "# Calculate F1 score on the test set\n",
        "test_f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "print(f\"Test F1 Score: {test_f1:.2f}\")\n",
        "\n",
        "# Calculate precision on the test set\n",
        "test_precision = precision_score(y_test, y_pred, average='weighted')\n",
        "print(f\"Test Precision: {test_precision:.2f}\")\n",
        "\n",
        "# Calculate recall on the test set\n",
        "test_recall = recall_score(y_test, y_pred, average='weighted')\n",
        "print(f\"Test Recall: {test_recall:.2f}\")\n"
      ],
      "metadata": {
        "id": "AgGZ0rG27yuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Random Search"
      ],
      "metadata": {
        "id": "p6toy9VXgph6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Define the parameter distribution for RandomizedSearchCV\n",
        "param_dist = {\n",
        "    'rf__n_estimators': [int(x) for x in np.linspace(start=200, stop=2000, num=10)],\n",
        "    'rf__max_features': ['sqrt', 'log2'],\n",
        "    'rf__max_depth': [int(x) for x in np.linspace(10, 300, num=20)] + [None],\n",
        "    'rf__min_samples_split': [2, 5, 10, 15],\n",
        "    'rf__min_samples_leaf': [1, 2, 4, 6],\n",
        "    'rf__bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "# Initialize the pipeline:Random Forest\n",
        "pipeline = Pipeline([\n",
        "    ('rf', RandomForestClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "# Initialize RandomizedSearchCV\n",
        "rf_random = RandomizedSearchCV(estimator=pipeline, param_distributions=param_dist,\n",
        "                               n_iter=100, cv=StratifiedKFold(5), verbose=2,\n",
        "                               random_state=42, n_jobs=-1, scoring='roc_auc')\n",
        "\n",
        "# Fit RandomizedSearchCV to the original training data\n",
        "rf_random.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters found by RandomizedSearchCV\n",
        "print(\"Best parameters found by RandomizedSearchCV:\")\n",
        "print(rf_random.best_params_)\n",
        "\n",
        "# Predict on the original test data\n",
        "y_pred = rf_random.best_estimator_.predict(X_test)\n",
        "y_prob = rf_random.best_estimator_.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluate the model with default threshold\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"ROC AUC: {roc_auc:.4f}\")\n"
      ],
      "metadata": {
        "id": "6RZWxRBnBSbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bayesian Optimization"
      ],
      "metadata": {
        "id": "NV9B0tvfihJT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the best parameters found from RandomizedSearchCV RF\n",
        "best_params_rf = {\n",
        "    'n_estimators': 800,\n",
        "    'min_samples_split': 10,\n",
        "    'min_samples_leaf': 1,\n",
        "    'max_features': 'sqrt',\n",
        "    'max_depth': 193,\n",
        "    'bootstrap': False,\n",
        "    'class_weight': 'balanced'\n",
        "}\n",
        "\n",
        "def objective(trial):\n",
        "    n_estimators = trial.suggest_int('n_estimators', max(100, best_params_rf['n_estimators'] - 200), best_params_rf['n_estimators'] + 200)\n",
        "    min_samples_split = trial.suggest_int('min_samples_split', max(2, best_params_rf['min_samples_split'] - 3), best_params_rf['min_samples_split'] + 3)\n",
        "    min_samples_leaf = trial.suggest_int('min_samples_leaf', max(1, best_params_rf['min_samples_leaf'] - 2), best_params_rf['min_samples_leaf'] + 2)\n",
        "    max_features = trial.suggest_categorical('max_features', ['sqrt', 'log2'])\n",
        "    max_depth = trial.suggest_categorical('max_depth', [None, 10, 50, 86, 100, 193, 300])\n",
        "    bootstrap = trial.suggest_categorical('bootstrap', [True, False])\n",
        "    class_weight = trial.suggest_categorical('class_weight', ['balanced', None])\n",
        "    criterion = trial.suggest_categorical('criterion', ['gini', 'entropy'])\n",
        "    min_impurity_decrease = trial.suggest_float('min_impurity_decrease', 0.0, 0.01)\n",
        "\n",
        "    # Initialize RandomForestClassifier with suggested hyperparameters\n",
        "    clf = RandomForestClassifier(\n",
        "        n_estimators=n_estimators,\n",
        "        min_samples_split=min_samples_split,\n",
        "        min_samples_leaf=min_samples_leaf,\n",
        "        max_features=max_features,\n",
        "        max_depth=max_depth,\n",
        "        bootstrap=bootstrap,\n",
        "        class_weight=class_weight,\n",
        "        criterion=criterion,\n",
        "        min_impurity_decrease=min_impurity_decrease,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    cv_scores = cross_val_score(clf, X_train, y_train, cv=5, scoring='roc_auc')\n",
        "    return np.mean(cv_scores)\n",
        "\n",
        "\n",
        "# Perform optimization with Optuna\n",
        "study = optuna.create_study(direction='maximize')\n",
        "\n",
        "# Enqueue the trial with the best parameters from RandomizedSearchCV\n",
        "study.enqueue_trial(best_params_rf)\n",
        "\n",
        "study.optimize(objective, n_trials=50)\n",
        "\n",
        "# Print the best parameters and best score from Optuna\n",
        "print(\"Best Parameters from Optuna:\", study.best_params)\n",
        "\n",
        "\n",
        "# Retrieve the best model and evaluate on the test set\n",
        "best_params_optuna = study.best_params\n",
        "best_clf = RandomForestClassifier(**best_params_optuna, random_state=50)\n",
        "best_clf.fit(X_train, y_train)\n",
        "y_pred = best_clf.predict(X_test)\n",
        "y_prob = best_clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluate on test set\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Test Precision: {precision:.4f}\")\n",
        "print(f\"Test Recall: {recall:.4f}\")\n",
        "print(f\"Test F1 Score: {f1:.4f}\")\n",
        "print(f\"Test ROC AUC: {roc_auc:.4f}\")\n"
      ],
      "metadata": {
        "id": "YZzMDqz-BpDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Best parameters from Bayesian Optimization RF\n",
        "best_params_rf = {\n",
        "    'n_estimators': 661,\n",
        "    'min_samples_split': 8,\n",
        "    'min_samples_leaf': 1,\n",
        "    'max_features': 'sqrt',\n",
        "    'bootstrap': False,\n",
        "    'criterion': 'gini',\n",
        "    'max_depth': 100\n",
        "}\n",
        "\n",
        "# Create the Random Forest classifier with the best parameters\n",
        "final_model_rf = RandomForestClassifier(**best_params_rf, random_state=50)\n",
        "\n",
        "# Perform cross-validation with 5 folds on the training data\n",
        "cv_scores = cross_val_score(final_model_rf, X_train, y_train, cv=5, scoring='roc_auc')\n",
        "print(\"Cross-validation scores:\")\n",
        "print(cv_scores)\n",
        "print(f\"Mean CV accuracy: {np.mean(cv_scores):.4f}\")\n",
        "\n",
        "# Train the final model on the training data\n",
        "final_model_rf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = final_model_rf.predict(X_test)\n",
        "best_clf.fit(X_train, y_train)\n",
        "y_pred = best_clf.predict(X_test)\n",
        "y_prob = best_clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate evaluation metrics on the test set\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Print the results for test data\n",
        "print(f'Test Accuracy: {accuracy:.4f}')\n",
        "print(f'Test Precision: {precision:.4f}')\n",
        "print(f'Test Recall: {recall:.4f}')\n",
        "print(f'Test F1-Score: {f1:.4f}')\n"
      ],
      "metadata": {
        "id": "TrQ2Ej25Cu5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Feature Selection RF (RFECV)"
      ],
      "metadata": {
        "id": "gHlcFIgkj_KI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(X, y, test_size=0.25, random_state=50)\n",
        "final_model_rf = RandomForestClassifier(**best_params_rf, random_state=50)\n",
        "\n",
        "# Define the RFECV selector\n",
        "selector_rf = RFECV(estimator=final_model_rf, step=9, cv=StratifiedKFold(10), scoring='roc_auc', verbose=2)\n",
        "\n",
        "# Fit the RFECV selector on the training data\n",
        "selector_rf.fit(X_train_rf, y_train_rf)\n",
        "\n",
        "# Print the optimal number of features\n",
        "print(f\"Optimal number of features: {selector_rf.n_features_}\")\n",
        "\n",
        "# Get the selected features\n",
        "selected_features_mask_rf = selector_rf.support_\n",
        "selected_features_rf = X.columns[selected_features_mask_rf]\n",
        "\n",
        "# Print the selected features\n",
        "print(\"Selected features:\", selected_features_rf)\n",
        "\n",
        "# Create a new DataFrame with the selected features\n",
        "RandomForest_gene = X[selected_features_rf]\n"
      ],
      "metadata": {
        "id": "lHRCPIY4NQ_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new DataFrame with the selected features for training and testing sets\n",
        "X_train_selected_rf = X_train_rf[selected_features_rf]\n",
        "X_test_selected_rf = X_test_rf[selected_features_rf]\n",
        "\n",
        "\n",
        "# Train the final Random Forest model with the selected features\n",
        "final_model_rf.fit(X_train_selected_rf, y_train_rf)\n",
        "\n",
        "# Perform cross-validation on the training set\n",
        "cv_scores = cross_val_score(final_model_rf, X_train_selected_rf, y_train_rf, cv=5, scoring='roc_auc')\n",
        "print(\"Cross-validation scores:\")\n",
        "print(cv_scores)\n",
        "print(f\"Mean CV accuracy: {np.mean(cv_scores):.4f}\")\n",
        "\n",
        "# Make predictions on the test set using the original selected features\n",
        "y_pred_prob_rf = final_model_rf.predict_proba(X_test_selected_rf)[:, 1]\n",
        "y_pred_rf = final_model_rf.predict(X_test_selected_rf)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy_rf = accuracy_score(y_test_rf, y_pred_rf)\n",
        "precision_rf = precision_score(y_test_rf, y_pred_rf, average='weighted')\n",
        "recall_rf = recall_score(y_test_rf, y_pred_rf, average='weighted')\n",
        "f1_rf = f1_score(y_test_rf, y_pred_rf, average='weighted')\n",
        "roc_auc_rf = roc_auc_score(y_test_rf, y_pred_prob_rf, average='weighted')\n",
        "\n",
        "# Calculate confusion matrix\n",
        "cm = confusion_matrix(y_test_rf, y_pred_rf)\n",
        "print(\"Confusion Matrix:\\n\", cm)\n",
        "\n",
        "# Calculate specificity\n",
        "if cm.shape == (2, 2):\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    specificity = tn / (tn + fp)\n",
        "    print(f'Test Specificity: {specificity:.2f}')\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(f'Random Forest Test ROC AUC: {roc_auc_rf:.2f}')\n",
        "print(f'Random Forest Test Accuracy: {accuracy_rf:.2f}')\n",
        "print(f'Random Forest Test Precision: {precision_rf:.2f}')\n",
        "print(f'Random Forest Test Recall: {recall_rf:.2f}')\n",
        "print(f'Random Forest Test F1-Score: {f1_rf:.2f}')\n"
      ],
      "metadata": {
        "id": "6dkKl2N2UHOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##95% CI"
      ],
      "metadata": {
        "id": "vz5KsSjjifDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bootstrap_ci(metric_func, y_true, y_pred, y_pred_prob=None, n_bootstraps=1000, alpha=0.05, **kwargs):\n",
        "    bootstrapped_scores = []\n",
        "    for i in range(n_bootstraps):\n",
        "        indices = resample(np.arange(len(y_true)), replace=True)\n",
        "        if y_pred_prob is None:\n",
        "            score = metric_func(y_true[indices], y_pred[indices], **kwargs)\n",
        "        else:\n",
        "            score = metric_func(y_true[indices], y_pred_prob[indices], **kwargs)\n",
        "        bootstrapped_scores.append(score)\n",
        "    sorted_scores = np.sort(bootstrapped_scores)\n",
        "    lower_bound = np.percentile(sorted_scores, 100 * (alpha / 2))\n",
        "    upper_bound = np.percentile(sorted_scores, 100 * (1 - alpha / 2))\n",
        "    return lower_bound, upper_bound\n",
        "\n",
        "# Bootstrap 95% confidence intervals for each metric, including 'average' parameter where necessary\n",
        "accuracy_ci = bootstrap_ci(accuracy_score, y_test_rf, y_pred_rf)\n",
        "precision_ci = bootstrap_ci(precision_score, y_test_rf, y_pred_rf, average='weighted')\n",
        "recall_ci = bootstrap_ci(recall_score, y_test_rf, y_pred_rf, average='weighted')\n",
        "f1_ci = bootstrap_ci(f1_score, y_test_rf, y_pred_rf, average='weighted')\n",
        "roc_auc_ci = bootstrap_ci(roc_auc_score, y_test_rf, y_pred_rf, y_pred_prob=y_pred_prob_rf)\n",
        "specificity_ci = proportion_confint(tn, tn + fp, alpha=0.05, method='normal')\n",
        "\n",
        "# Print evaluation metrics with their 95% confidence intervals\n",
        "print(f'Random Forest Test ROC AUC: {roc_auc_rf:.2f} (95% CI: [{roc_auc_ci[0]:.2f}, {roc_auc_ci[1]:.2f}])')\n",
        "print(f'Random Forest Test Accuracy: {accuracy_rf:.2f} (95% CI: [{accuracy_ci[0]:.2f}, {accuracy_ci[1]:.2f}])')\n",
        "print(f'Random Forest Test Precision: {precision_rf:.2f} (95% CI: [{precision_ci[0]:.2f}, {precision_ci[1]:.2f}])')\n",
        "print(f'Random Forest Test Recall: {recall_rf:.2f} (95% CI: [{recall_ci[0]:.2f}, {recall_ci[1]:.2f}])')\n",
        "print(f'Random Forest Test F1-Score: {f1_rf:.2f} (95% CI: [{f1_ci[0]:.2f}, {f1_ci[1]:.2f}])')\n",
        "print(f'Random Forest Test Specificity: {specificity:.2f} (95% CI: [{specificity_ci[0]:.2f}, {specificity_ci[1]:.2f}])')\n"
      ],
      "metadata": {
        "id": "87ZFCjBikUia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LASSO"
      ],
      "metadata": {
        "id": "9IK16t5nmFQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = mtb.drop(['Study.Group'], axis=1)\n",
        "y = mtb['Study.Group']\n",
        "\n",
        "# Encode categorical target labels into numerical labels\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Create Logistic Regression classifier with L1 regularization (Lasso)\n",
        "log_reg = LogisticRegression(penalty='l1', solver='liblinear', random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = log_reg.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'F1 Score: {f1:.4f}')\n"
      ],
      "metadata": {
        "id": "_J37h0XxkU80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Random Search"
      ],
      "metadata": {
        "id": "ghVYOf8_InSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Define the parameter grid for Randomized Search\n",
        "param_dist = {\n",
        "    'C': np.logspace(-4, 4, 20),\n",
        "    'solver': ['liblinear'],\n",
        "    'max_iter': [1000, 5000, 10000, 20000],\n",
        "    'tol': [1e-4, 1e-3, 1e-2, 1e-1]\n",
        "}\n",
        "\n",
        "# Create Logistic Regression classifier with L1 regularization\n",
        "log_reg = LogisticRegression(penalty='l1', random_state=42)\n",
        "\n",
        "# Set up the Randomized Search with cross-validation\n",
        "random_search = RandomizedSearchCV(\n",
        "    log_reg, param_distributions=param_dist, n_iter=100,\n",
        "    scoring='accuracy', cv=5, verbose=1, random_state=42, n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit the Randomized Search model\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "best_params = random_search.best_params_\n",
        "print(\"Best Parameters:\", best_params)\n",
        "\n",
        "# Predict on the test set with the best model\n",
        "y_pred = random_search.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'F1 Score: {f1:.4f}')\n"
      ],
      "metadata": {
        "id": "GvZyktApAjoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bayesian Optimization"
      ],
      "metadata": {
        "id": "c4tCqeUoI5Vv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjust Random Search best parameters\n",
        "random_search_params = {'tol': 0.0001, 'solver': 'liblinear', 'max_iter': 20000, 'C':1438.44988828766}\n",
        "\n",
        "def objective(trial):\n",
        "    # Expand the upper bound of C to accommodate the value from the random search\n",
        "    C = trial.suggest_float('C', 1e-4, 1e4, log=True)  # Increased upper limit\n",
        "    max_iter = trial.suggest_int('max_iter', 1000, 50000)\n",
        "    tol = trial.suggest_float('tol', 1e-4, 1e-2, log=True)\n",
        "    solver = trial.suggest_categorical('solver', ['liblinear', 'saga'])\n",
        "\n",
        "    # Create a pipeline with Logistic Regression\n",
        "    clf = make_pipeline(\n",
        "        LogisticRegression(\n",
        "            penalty='l1', C=C, max_iter=max_iter, tol=tol, solver=solver, random_state=42\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Cross-validation to compute AUC-ROC score\n",
        "    score = cross_val_score(clf, X_train, y_train, cv=5, scoring='roc_auc').mean()\n",
        "\n",
        "    return score\n",
        "\n",
        "\n",
        "# Perform optimization with Optuna, initializing with Random Search parameters\n",
        "study = optuna.create_study(direction='maximize')\n",
        "\n",
        "# Set the Random Search best parameters as the first trial\n",
        "def random_search_trial(trial):\n",
        "    trial.suggest_float('C', 100.0, 100.0)\n",
        "    trial.suggest_int('max_iter', 20000, 20000)\n",
        "    trial.suggest_float('tol', 0.0001, 0.0001)\n",
        "    trial.suggest_categorical('solver', ['liblinear'])\n",
        "\n",
        "study.enqueue_trial(random_search_params)\n",
        "study.optimize(objective, n_trials=50)\n",
        "\n",
        "# Print the best parameters and best AUC-ROC score from Optuna\n",
        "print(\"Best Parameters from Optuna:\", study.best_params)\n",
        "print(\"Best AUC-ROC Score from Optuna:\", study.best_value)\n",
        "\n",
        "# Retrieve the best parameters and train the model on the balanced data\n",
        "best_params = study.best_params\n",
        "\n",
        "# Pass the best parameters to LogisticRegression\n",
        "clf = LogisticRegression(\n",
        "    penalty='l1',\n",
        "    C=best_params['C'],\n",
        "    max_iter=best_params['max_iter'],\n",
        "    tol=best_params['tol'],\n",
        "    solver=best_params['solver'],\n",
        "    random_state=42\n",
        ")\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_prob = clf.predict_proba(X_test)[:, 1]\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(f\"Test AUC-ROC: {roc_auc:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Test Precision: {precision:.4f}\")\n",
        "print(f\"Test Recall: {recall:.4f}\")\n",
        "print(f\"Test F1 Score: {f1:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "AVQprxjwbNJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_lasso, X_test_lasso, y_train_lasso, y_test_lasso = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Best parameters from Bayesian Optimization with Optuna\n",
        "best_params_lasso = {\n",
        "    'C': 512.0959297957417,\n",
        "    'max_iter':30362,\n",
        "    'tol': 0.0005183129895961714,\n",
        "    'solver': 'liblinear'\n",
        "\n",
        "}\n",
        "# Use Lasso with class weights\n",
        "lasso_model = LogisticRegression(penalty='l1', class_weight='balanced', **best_params_lasso, random_state=42)\n",
        "\n",
        "# Perform cross-validation\n",
        "cv_scores = cross_val_score(lasso_model, X_train_lasso, y_train_lasso, cv=5, scoring='roc_auc')\n",
        "print(\"Cross-validation:\", cv_scores)\n",
        "print(f\"Mean CV: {np.mean(cv_scores):.4f}\")\n",
        "\n",
        "# Train the Lasso model on the entire training data\n",
        "lasso_model.fit(X_train_lasso, y_train_lasso)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_lasso = lasso_model.predict(X_test_lasso)\n",
        "\n",
        "# Calculate evaluation metrics on the test set\n",
        "accuracy = accuracy_score(y_test_lasso, y_pred_lasso)\n",
        "precision = precision_score(y_test_lasso, y_pred_lasso, average='weighted')\n",
        "recall = recall_score(y_test_lasso, y_pred_lasso, average='weighted')\n",
        "f1 = f1_score(y_test_lasso, y_pred_lasso, average='weighted')\n",
        "\n",
        "# Print the results for test data\n",
        "print(f'Test Accuracy: {accuracy:.4f}')\n",
        "print(f'Test Precision: {precision:.4f}')\n",
        "print(f'Test Recall: {recall:.4f}')\n",
        "print(f'Test F1-Score: {f1:.4f}')\n"
      ],
      "metadata": {
        "id": "VpIp-R9DKAI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##LASSO Feature Selection"
      ],
      "metadata": {
        "id": "hHr0ocjXinQ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lasso_model = LogisticRegression(penalty='l1', **best_params_lasso, random_state=42)\n",
        "\n",
        "# Fit the final model on the entire training data\n",
        "lasso_model.fit(X_train_lasso, y_train_lasso)\n",
        "\n",
        "# Extracting feature names from original DataFrame\n",
        "feature_names = X.columns\n",
        "\n",
        "#Print out selected features based on non-zero coefficients\n",
        "selected_features_lasso = feature_names[np.abs(lasso_model.coef_[0]) > 0]\n",
        "selected_coefficients = lasso_model.coef_[0][np.abs(lasso_model.coef_[0]) > 0]\n",
        "\n",
        "print(\"Selected features and coefficients:\")\n",
        "for feature, coef in zip(selected_features_lasso, selected_coefficients):\n",
        "    print(f\"{feature}: {coef:.4f}\")\n",
        "\n",
        "# Optionally, print the number of selected features\n",
        "print(f\"Number of selected features: {len(selected_features_lasso)}\")"
      ],
      "metadata": {
        "id": "ZPHbDpy00vUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_model_lasso = LogisticRegression(penalty='l1', **best_params_lasso, random_state=42)\n",
        "final_model_lasso.fit(X_train_lasso, y_train_lasso)\n",
        "\n",
        "# Make predictions on the test set (predicted probabilities)\n",
        "y_pred_proba_lasso = final_model_lasso.predict_proba(X_test_lasso)[:, 1]\n",
        "\n",
        "# Convert probabilities to predicted class labels\n",
        "y_pred_lasso = final_model_lasso.predict(X_test_lasso)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test_lasso, y_pred_lasso)\n",
        "precision = precision_score(y_test_lasso, y_pred_lasso)\n",
        "recall = recall_score(y_test_lasso, y_pred_lasso)\n",
        "f1 = f1_score(y_test_lasso, y_pred_lasso)\n",
        "roc_auc = roc_auc_score(y_test_lasso, y_pred_proba_lasso)\n",
        "\n",
        "# Calculate confusion matrix and extract TN, FP, FN, TP\n",
        "tn, fp, fn, tp = confusion_matrix(y_test_lasso, y_pred_lasso).ravel()\n",
        "\n",
        "# Calculate specificity\n",
        "specificity = tn / (tn + fp)\n",
        "\n",
        "print(f'Test ROC AUC: {roc_auc:.2f}')\n",
        "print(f'Test Accuracy: {accuracy:.2f}')\n",
        "print(f'Test Precision: {precision:.2f}')\n",
        "print(f'Test Recall: {recall:.2f}')\n",
        "print(f'Test F1-Score: {f1:.2f}')\n",
        "print(f'Test Specificity: {specificity:.2f}')"
      ],
      "metadata": {
        "id": "e59FdT10MRuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##95% CI"
      ],
      "metadata": {
        "id": "diurVQZQit_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate 95% Confidence Intervals using bootstrapping\n",
        "def bootstrap_ci(metric_func, y_true, y_pred, y_proba=None, n_bootstraps=1000, alpha=0.95):\n",
        "    bootstrapped_scores = []\n",
        "    n_size = len(y_true)\n",
        "\n",
        "    for i in range(n_bootstraps):\n",
        "        # Resample the test data\n",
        "        indices = resample(range(n_size), replace=True, n_samples=n_size, random_state=i)\n",
        "        y_true_resampled = y_true[indices]\n",
        "        y_pred_resampled = y_pred[indices]\n",
        "\n",
        "        # If the metric requires probabilities (ROC AUC), pass them\n",
        "        if y_proba is not None:\n",
        "            y_proba_resampled = y_proba[indices]\n",
        "            score = metric_func(y_true_resampled, y_proba_resampled)\n",
        "        else:\n",
        "            score = metric_func(y_true_resampled, y_pred_resampled)\n",
        "\n",
        "        bootstrapped_scores.append(score)\n",
        "\n",
        "    # Compute the confidence interval\n",
        "    sorted_scores = np.sort(bootstrapped_scores)\n",
        "    lower_bound = np.percentile(sorted_scores, (1 - alpha) / 2 * 100)\n",
        "    upper_bound = np.percentile(sorted_scores, (1 + alpha) / 2 * 100)\n",
        "\n",
        "    return lower_bound, upper_bound\n",
        "\n",
        "# Apply bootstrap CI for each metric\n",
        "accuracy_ci = bootstrap_ci(accuracy_score, y_test_lasso, y_pred_lasso)\n",
        "precision_ci = bootstrap_ci(precision_score, y_test_lasso, y_pred_lasso)\n",
        "recall_ci = bootstrap_ci(recall_score, y_test_lasso, y_pred_lasso)\n",
        "f1_ci = bootstrap_ci(f1_score, y_test_lasso, y_pred_lasso)\n",
        "roc_auc_ci = bootstrap_ci(roc_auc_score, y_test_lasso, y_pred_lasso, y_proba=y_pred_proba_lasso)\n",
        "\n",
        "# Specificity requires calculating from confusion matrix, so custom handling\n",
        "def specificity_metric(y_true, y_pred):\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    return tn / (tn + fp)\n",
        "\n",
        "specificity_ci = bootstrap_ci(specificity_metric, y_test_lasso, y_pred_lasso)\n",
        "\n",
        "# Print evaluation metrics with their 95% Confidence Intervals\n",
        "print(f'Test ROC AUC: {roc_auc:.2f} (95% CI: {roc_auc_ci[0]:.2f}, {roc_auc_ci[1]:.2f})')\n",
        "print(f'Test Accuracy: {accuracy:.2f} (95% CI: {accuracy_ci[0]:.2f}, {accuracy_ci[1]:.2f})')\n",
        "print(f'Test Precision: {precision:.2f} (95% CI: {precision_ci[0]:.2f}, {precision_ci[1]:.2f})')\n",
        "print(f'Test Recall: {recall:.2f} (95% CI: {recall_ci[0]:.2f}, {recall_ci[1]:.2f})')\n",
        "print(f'Test F1-Score: {f1:.2f} (95% CI: {f1_ci[0]:.2f}, {f1_ci[1]:.2f})')\n",
        "print(f'Test Specificity: {specificity:.2f} (95% CI: {specificity_ci[0]:.2f}, {specificity_ci[1]:.2f})')\n"
      ],
      "metadata": {
        "id": "LYJKaj5RgRoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###COMMON FEATURES BETWEEN XGBOOST, RANDOM FOREST AND LASSO"
      ],
      "metadata": {
        "id": "z-t3IrkEtR08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count rows in each DataFrame\n",
        "count_XGBoost = selected_features_xg.shape[0]\n",
        "count_rf = selected_features_rf.shape[0]\n",
        "count_df = selected_features_lasso.shape[0]\n",
        "\n",
        "# Print counts\n",
        "print(f\"Number of rows in selected_features_XGBoost: {count_XGBoost}\")\n",
        "print(f\"Number of rows in selected_features_rf: {count_rf}\")\n",
        "print(f\"Number of rows in selected_features_lasso: {count_df}\")"
      ],
      "metadata": {
        "id": "bpbz50IYZtsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract feature lists from DataFrames\n",
        "features_XGBoost = set(selected_features_xg)\n",
        "features_rf = set(selected_features_rf)\n",
        "features_df = set(selected_features_lasso)\n",
        "\n",
        "# Find common features using set intersection\n",
        "common_features = features_XGBoost & features_rf & features_df\n",
        "\n",
        "# Print common features\n",
        "print(\"Common Features:\")\n",
        "for feature in sorted(common_features):\n",
        "    print(feature)\n",
        "\n",
        "# Count the number of common features\n",
        "num_common_features = len(common_features)\n",
        "print(f\"\\nNumber of common features: {num_common_features}\")"
      ],
      "metadata": {
        "id": "UTc0zs4M5mGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #Extract feature lists from DataFrames\n",
        "features_XGBoost = set(selected_features_xg)\n",
        "features_rf = set(selected_features_rf)\n",
        "\n",
        "\n",
        "# Find common features using set intersection\n",
        "common_features = features_XGBoost & features_rf\n",
        "\n",
        "# Print common features\n",
        "print(\"Common Features:\")\n",
        "for feature in sorted(common_features):\n",
        "    print(feature)\n",
        "\n",
        "# Count the number of common features\n",
        "num_common_features = len(common_features)\n",
        "print(f\"\\nNumber of common features: {num_common_features}\")"
      ],
      "metadata": {
        "id": "HlLYXKh6S40J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract feature lists from DataFrames\n",
        "features_XGBoost = set(selected_features_xg)\n",
        "\n",
        "features_df = set(selected_features_lasso)\n",
        "\n",
        "# Find common features using set intersection\n",
        "common_features = features_XGBoost & features_df\n",
        "\n",
        "# Print common features\n",
        "print(\"Common Features:\")\n",
        "for feature in sorted(common_features):\n",
        "    print(feature)\n",
        "\n",
        "# Count the number of common features\n",
        "num_common_features = len(common_features)\n",
        "print(f\"\\nNumber of common features: {num_common_features}\")"
      ],
      "metadata": {
        "id": "QxpYJLfWS8kN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract feature lists from DataFrames\n",
        "\n",
        "features_rf = set(selected_features_rf)\n",
        "features_df = set(selected_features_lasso)\n",
        "\n",
        "# Find common features using set intersection\n",
        "common_features = features_rf & features_df\n",
        "\n",
        "# Print common features\n",
        "print(\"Common Features:\")\n",
        "for feature in sorted(common_features):\n",
        "    print(feature)\n",
        "\n",
        "# Count the number of common features\n",
        "num_common_features = len(common_features)\n",
        "print(f\"\\nNumber of common features: {num_common_features}\")"
      ],
      "metadata": {
        "id": "BIYOrJhmTBHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###AUC-ROC CURVE\n"
      ],
      "metadata": {
        "id": "lu6RSd-9DAsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_selected_xg = selector.transform(X_train_xg)\n",
        "X_test_selected_xg = selector.transform(X_test)\n",
        "\n",
        "# Train the final model with the selected features on the training set\n",
        "final_model_xg.fit(X_train_selected_xg, y_train_xg)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_prob_xg = final_model_xg.predict_proba(X_test_selected_xg)[:, 1]\n",
        "y_pred_xg = final_model_xg.predict(X_test_selected_xg)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test_xg, y_pred_xg)\n",
        "precision = precision_score(y_test_xg, y_pred_xg)\n",
        "recall = recall_score(y_test_xg, y_pred_xg)\n",
        "f1 = f1_score(y_test_xg, y_pred_xg)\n",
        "roc_auc = roc_auc_score(y_test_xg, y_pred_prob_xg)\n",
        "\n",
        "# Calculate specificity\n",
        "tn, fp, fn, tp = confusion_matrix(y_test_xg, y_pred_xg).ravel()\n",
        "specificity = tn / (tn + fp)\n",
        "\n",
        "# Print the results\n",
        "print(f'Test ROC AUC: {roc_auc:.2f}')\n",
        "print(f'Test Accuracy: {accuracy:.2f}')\n",
        "print(f'Test Precision: {precision:.2f}')\n",
        "print(f'Test Recall: {recall:.2f}')\n",
        "print(f'Test F1-Score: {f1:.2f}')\n",
        "print(f'Test Specificity: {specificity:.2f}')"
      ],
      "metadata": {
        "id": "rb6D_30DFxR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Random Forest\n",
        "X_train_selected_rf = X_train_rf[selected_features_rf]\n",
        "X_test_selected_rf = X_test_rf[selected_features_rf]\n",
        "\n",
        "# Train the final Random Forest model with the selected features\n",
        "final_model_rf.fit(X_train_selected_rf, y_train_rf)\n",
        "\n",
        "# Perform cross-validation on the training set\n",
        "cv_scores = cross_val_score(final_model_rf, X_train_selected_rf, y_train_rf, cv=5, scoring='roc_auc')\n",
        "print(\"Cross-validation scores:\")\n",
        "print(cv_scores)\n",
        "print(f\"Mean CV accuracy: {np.mean(cv_scores):.4f}\")\n",
        "\n",
        "# Make predictions on the test set using the original selected features\n",
        "y_pred_prob_rf = final_model_rf.predict_proba(X_test_selected_rf)[:, 1]\n",
        "y_pred_rf = final_model_rf.predict(X_test_selected_rf)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy_rf = accuracy_score(y_test_rf, y_pred_rf)\n",
        "precision_rf = precision_score(y_test_rf, y_pred_rf, average='weighted')\n",
        "recall_rf = recall_score(y_test_rf, y_pred_rf, average='weighted')\n",
        "f1_rf = f1_score(y_test_rf, y_pred_rf, average='weighted')\n",
        "roc_auc_rf = roc_auc_score(y_test_rf, y_pred_prob_rf, average='weighted')\n",
        "\n",
        "# Calculate confusion matrix\n",
        "cm = confusion_matrix(y_test_rf, y_pred_rf)\n",
        "print(\"Confusion Matrix:\\n\", cm)\n",
        "\n",
        "# Calculate specificity\n",
        "if cm.shape == (2, 2):\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    specificity = tn / (tn + fp)\n",
        "    print(f'Test Specificity: {specificity:.2f}')\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(f'Random Forest Test ROC AUC: {roc_auc_rf:.2f}')\n",
        "print(f'Random Forest Test Accuracy: {accuracy_rf:.2f}')\n",
        "print(f'Random Forest Test Precision: {precision_rf:.2f}')\n",
        "print(f'Random Forest Test Recall: {recall_rf:.2f}')\n",
        "print(f'Random Forest Test F1-Score: {f1_rf:.2f}')\n"
      ],
      "metadata": {
        "id": "1TUFnPc8Fy6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LASSO\n",
        "# Train the Logistic Regression model again using only the selected features\n",
        "final_model_lasso = LogisticRegression(penalty='l1', **best_params_lasso, random_state=42)\n",
        "final_model_lasso.fit(X_train_lasso, y_train_lasso)\n",
        "\n",
        "# Make predictions on the test set (predicted probabilities)\n",
        "y_pred_proba_lasso = final_model_lasso.predict_proba(X_test_lasso)[:, 1]\n",
        "\n",
        "# Convert probabilities to predicted class labels\n",
        "y_pred_lasso = final_model_lasso.predict(X_test_lasso)\n",
        "\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test_lasso, y_pred_lasso)\n",
        "precision = precision_score(y_test_lasso, y_pred_lasso)\n",
        "recall = recall_score(y_test_lasso, y_pred_lasso)\n",
        "f1 = f1_score(y_test_lasso, y_pred_lasso)\n",
        "roc_auc = roc_auc_score(y_test_lasso, y_pred_proba_lasso)\n",
        "\n",
        "# Calculate confusion matrix and extract TN, FP, FN, TP\n",
        "tn, fp, fn, tp = confusion_matrix(y_test_lasso, y_pred_lasso).ravel()\n",
        "\n",
        "# Calculate specificity\n",
        "specificity = tn / (tn + fp)\n",
        "\n",
        "print(f'Test ROC AUC: {roc_auc:.2f}')\n",
        "print(f'Test Accuracy: {accuracy:.2f}')\n",
        "print(f'Test Precision: {precision:.2f}')\n",
        "print(f'Test Recall: {recall:.2f}')\n",
        "print(f'Test F1-Score: {f1:.2f}')\n",
        "print(f'Test Specificity: {specificity:.2f}')"
      ],
      "metadata": {
        "id": "RVMHw8jTVW93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate ROC-AUC with 95% confidence intervals\n",
        "def calc_auc_ci(y_true, y_pred_prob, n_bootstraps=1000, ci_level=0.95):\n",
        "    bootstrapped_scores = []\n",
        "    rng = np.random.RandomState(50)\n",
        "    for _ in range(n_bootstraps):\n",
        "        indices = rng.randint(0, len(y_pred_prob), len(y_pred_prob))\n",
        "        if len(np.unique(y_true[indices])) < 2:\n",
        "            continue\n",
        "        score = roc_auc_score(y_true[indices], y_pred_prob[indices])\n",
        "        bootstrapped_scores.append(score)\n",
        "    sorted_scores = np.array(bootstrapped_scores)\n",
        "    sorted_scores.sort()\n",
        "    ci_lower = sorted_scores[int((1.0 - ci_level) / 2.0 * len(sorted_scores))]\n",
        "    ci_upper = sorted_scores[int((1.0 + ci_level) / 2.0 * len(sorted_scores))]\n",
        "    return ci_lower, ci_upper\n",
        "\n",
        "# XGBoost Model\n",
        "final_model_xg = xgb.XGBClassifier(**best_params_xg)\n",
        "final_model_xg.fit(X_train_selected_xg, y_train_xg)\n",
        "y_pred_prob_xg = final_model_xg.predict_proba(X_test_selected_xg)[:, 1]\n",
        "fpr_xg, tpr_xg, _ = roc_curve(y_test_xg, y_pred_prob_xg)\n",
        "roc_auc_xg = auc(fpr_xg, tpr_xg)\n",
        "ci_lower_xg, ci_upper_xg = calc_auc_ci(y_test_xg, y_pred_prob_xg)\n",
        "\n",
        "# Random Forest Model\n",
        "final_model_rf.fit(X_train_selected_rf, y_train_rf)\n",
        "y_pred_prob_rf = final_model_rf.predict_proba(X_test_selected_rf)[:, 1]\n",
        "fpr_rf, tpr_rf, _ = roc_curve(y_test_rf, y_pred_prob_rf)\n",
        "roc_auc_rf = auc(fpr_rf, tpr_rf)\n",
        "ci_lower_rf, ci_upper_rf = calc_auc_ci(y_test_rf, y_pred_prob_rf)\n",
        "\n",
        "# Lasso (Logistic Regression with L1 penalty)\n",
        "final_model_lasso = LogisticRegression(penalty='l1', **best_params_lasso, random_state=42)\n",
        "final_model_lasso.fit(X_train_lasso, y_train_lasso)\n",
        "y_pred_prob_lasso = final_model_lasso.predict_proba(X_test_lasso)[:, 1]\n",
        "fpr_lasso, tpr_lasso, _ = roc_curve(y_test_lasso, y_pred_prob_lasso)\n",
        "roc_auc_lasso = auc(fpr_lasso, tpr_lasso)\n",
        "ci_lower_lasso, ci_upper_lasso = calc_auc_ci(y_test_lasso, y_pred_prob_lasso)\n",
        "\n",
        "# Plot ROC curves with confidence intervals\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# XGBoost\n",
        "plt.plot(fpr_xg, tpr_xg, color='blue', lw=2, label=f'XGBoost (AUC = {roc_auc_xg:.2f} [{ci_lower_xg:.2f}-{ci_upper_xg:.2f}])')\n",
        "plt.fill_between(fpr_xg, tpr_xg - (roc_auc_xg - ci_lower_xg), tpr_xg + (ci_upper_xg - roc_auc_xg), color='blue', alpha=0.2)\n",
        "\n",
        "# Random Forest\n",
        "plt.plot(fpr_rf, tpr_rf, color='green', lw=2, label=f'Random Forest (AUC = {roc_auc_rf:.2f} [{ci_lower_rf:.2f}-{ci_upper_rf:.2f}])')\n",
        "plt.fill_between(fpr_rf, tpr_rf - (roc_auc_rf - ci_lower_rf), tpr_rf + (ci_upper_rf - roc_auc_rf), color='green', alpha=0.2)\n",
        "\n",
        "# Lasso\n",
        "plt.plot(fpr_lasso, tpr_lasso, color='red', lw=2, label=f'LASSO (AUC = {roc_auc_lasso:.2f} [{ci_lower_lasso:.2f}-{ci_upper_lasso:.2f}])')\n",
        "plt.fill_between(fpr_lasso, tpr_lasso - (roc_auc_lasso - ci_lower_lasso), tpr_lasso + (ci_upper_lasso - roc_auc_lasso), color='red', alpha=0.2)\n",
        "\n",
        "# Plot the no skill line\n",
        "plt.plot([0, 1], [0, 1], color='grey', lw=2, linestyle='--', label='No Skill')\n",
        "\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate', fontsize=16)\n",
        "plt.ylabel('True Positive Rate', fontsize=16)\n",
        "plt.title('ROC-AUC Curve for Metabolites (IBD)', fontsize=18)\n",
        "plt.legend(loc=\"lower right\", fontsize=12)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "svBaMBDmQj8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Close previous figures\n",
        "plt.close('all')\n",
        "\n",
        "# Plot ROC curves with confidence intervals\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# XGBoost\n",
        "plt.plot(fpr_xg, tpr_xg, color='blue', lw=2, label=f'XGBoost (AUC = {roc_auc_xg:.2f} [{ci_lower_xg:.2f}-{ci_upper_xg:.2f}])')\n",
        "plt.fill_between(fpr_xg, tpr_xg - (roc_auc_xg - ci_lower_xg), tpr_xg + (ci_upper_xg - roc_auc_xg), color='blue', alpha=0.2)\n",
        "\n",
        "# Random Forest\n",
        "plt.plot(fpr_rf, tpr_rf, color='green', lw=2, label=f'Random Forest (AUC = {roc_auc_rf:.2f} [{ci_lower_rf:.2f}-{ci_upper_rf:.2f}])')\n",
        "plt.fill_between(fpr_rf, tpr_rf - (roc_auc_rf - ci_lower_rf), tpr_rf + (ci_upper_rf - roc_auc_rf), color='green', alpha=0.2)\n",
        "\n",
        "# Lasso (Logistic Regression)\n",
        "plt.plot(fpr_lasso, tpr_lasso, color='red', lw=2, label=f'LASSO (AUC = {roc_auc_lasso:.2f} [{ci_lower_lasso:.2f}-{ci_upper_lasso:.2f}])')\n",
        "plt.fill_between(fpr_lasso, tpr_lasso - (roc_auc_lasso - ci_lower_lasso), tpr_lasso + (ci_upper_lasso - roc_auc_lasso), color='red', alpha=0.2)\n",
        "\n",
        "# Plot the no skill line\n",
        "plt.plot([0, 1], [0, 1], color='grey', lw=2, linestyle='--', label='No Skill')\n",
        "\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate', fontsize=16)\n",
        "plt.ylabel('True Positive Rate', fontsize=16)\n",
        "plt.title('ROC-AUC Curve for Metabolites (IBD)', fontsize=18)\n",
        "plt.legend(loc=\"lower right\", fontsize=12)\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "# Save the plot before displaying it\n",
        "plt.savefig('ROC-AUC_Curve_For_IBD_Metabolites_final.png', dpi=600, bbox_inches='tight')\n",
        "\n",
        "# Now display the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "f7PSUcirgXJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Convert the selected feature lists to sets\n",
        "features_XGBoost = set(selected_features_xg)\n",
        "features_rf = set(selected_features_rf)\n",
        "features_df = set(selected_features_lasso)\n",
        "\n",
        "# Find common features using set intersection\n",
        "common_features = features_XGBoost & features_rf & features_df\n",
        "\n",
        "# Print common features\n",
        "print(\"Common Features:\")\n",
        "for feature in sorted(common_features):\n",
        "    print(feature)\n",
        "\n",
        "# Count the number of common features\n",
        "num_common_features = len(common_features)\n",
        "print(f\"\\nNumber of common features: {num_common_features}\")\n",
        "\n",
        "# Extract values of common features from the 'gene' DataFrame\n",
        "common_features_values = mtb[list(common_features)]\n",
        "\n",
        "# Print the DataFrame containing the common features and their values\n",
        "print(\"\\nValues of Common Features:\")\n",
        "print(common_features_values)\n",
        "\n",
        "# Save the DataFrame to a excel file\n",
        "common_features_values.to_excel('IBD Metabolites Final.xlsx', index=False)"
      ],
      "metadata": {
        "id": "6SE0DvceAbvj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}