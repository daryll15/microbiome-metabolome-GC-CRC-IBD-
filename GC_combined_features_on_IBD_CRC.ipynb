{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###Import necessary libraries"
      ],
      "metadata": {
        "id": "_H6W9nPiziog"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "xUfAXTJAq8ET"
      },
      "outputs": [],
      "source": [
        "! pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "import xgboost\n",
        "\n",
        "print(f\"Scikit-learn version: {sklearn.__version__}\")\n",
        "print(f\"XGBoost version: {xgboost.__version__}\")\n"
      ],
      "metadata": {
        "id": "zBXB5bVUyb1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall scikit-learn -y\n",
        "!pip install scikit-learn==1.5.2"
      ],
      "metadata": {
        "id": "1SeaxMgCyfGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TD-V3kRPTdU1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import os,os.path\n",
        "import re\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "import optuna\n",
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.utils import resample\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from scipy.stats import bootstrap\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Load the datasets"
      ],
      "metadata": {
        "id": "Om5Tl0oGetyv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gc = pd.read_excel('GC biomarkers reduced.xlsx')\n",
        "gc"
      ],
      "metadata": {
        "id": "bGSEW4Zkx0rx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cc = pd.read_excel('GC biomarkers in CC.xlsx')\n",
        "cc"
      ],
      "metadata": {
        "id": "FhJeaoXQyH92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ibd = pd.read_excel('GC biomarkers in IBD.xlsx')\n",
        "ibd"
      ],
      "metadata": {
        "id": "AS0ySQ9gyC5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select all columns except 'group'\n",
        "features = gc.drop(columns=['Group'])\n",
        "\n",
        "# Initialize the Min-Max Scaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Apply Min-Max scaling to the selected features\n",
        "scaled_features = scaler.fit_transform(features)\n",
        "\n",
        "# Replace the original feature columns with the scaled ones\n",
        "gc = gc.copy()\n",
        "gc.loc[:, features.columns] = scaled_features\n",
        "\n",
        "# Display the first few rows of the scaled DataFrame\n",
        "print(gc.head())\n"
      ],
      "metadata": {
        "id": "SDbIdk_UzGk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select all columns except 'group'\n",
        "features = cc.drop(columns=['Group'])\n",
        "\n",
        "# Initialize the Min-Max Scaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Apply Min-Max scaling to the selected features\n",
        "scaled_features = scaler.fit_transform(features)\n",
        "\n",
        "# Replace the original feature columns with the scaled ones\n",
        "cc = cc.copy()\n",
        "cc.loc[:, features.columns] = scaled_features\n",
        "\n",
        "# Display the first few rows of the scaled DataFrame\n",
        "print(cc.head())\n"
      ],
      "metadata": {
        "id": "j2M9ndlByqSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select all columns except 'group'\n",
        "features = ibd.drop(columns=['Group'])\n",
        "\n",
        "# Initialize the Min-Max Scaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Apply Min-Max scaling to the selected features\n",
        "scaled_features = scaler.fit_transform(features)\n",
        "\n",
        "# Replace the original feature columns with the scaled ones\n",
        "ibd = ibd.copy()\n",
        "ibd.loc[:, features.columns] = scaled_features\n",
        "\n",
        "# Display the first few rows of the scaled DataFrame\n",
        "print(ibd.head())\n"
      ],
      "metadata": {
        "id": "dajhZXtYzPoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#XGBoost"
      ],
      "metadata": {
        "id": "YN9CAgSbzmOF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = gc.drop(['Group'], axis=1)\n",
        "y = gc['Group']\n",
        "# Encode categorical target labels into numerical labels\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "\n",
        "#Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Initialize XGBoost classifier with default parameters\n",
        "model = xgb.XGBClassifier()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "print(f\"Test Precision: {precision:.4f}\")\n",
        "print(f\"Test F1 Score: {f1:.4f}\")\n",
        "print(f\"Test Recall: {recall:.4f}\")"
      ],
      "metadata": {
        "id": "mHL9cAsXCbMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Random Search"
      ],
      "metadata": {
        "id": "kQykMNAolD7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Define the parameter grid\n",
        "param_dist = {\n",
        "    'n_estimators': np.arange(50, 200, 10),\n",
        "    'max_depth': np.arange(3, 10),\n",
        "    'learning_rate': np.linspace(0.01, 0.3, 10),\n",
        "    'subsample': np.linspace(0.5, 1.0, 10),\n",
        "    'colsample_bytree': np.linspace(0.5, 1.0, 10),\n",
        "    'gamma': np.linspace(0, 0.5, 5),\n",
        "    'min_child_weight': np.arange(1, 6),\n",
        "    'reg_alpha': np.linspace(0, 1, 20),\n",
        "    'reg_lambda': np.linspace(0, 1, 20),\n",
        "}\n",
        "\n",
        "# Initialize the model\n",
        "xgb = XGBClassifier()\n",
        "\n",
        "# Randomized search\n",
        "random_search = RandomizedSearchCV(\n",
        "    xgb, param_distributions=param_dist, n_iter=100,\n",
        "    scoring='roc_auc', cv=5, verbose=1, random_state=42, n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "best_params = random_search.best_params_\n",
        "print(\"Best Parameters:\", best_params)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = random_search.predict(X_test)\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1 Score: {f1:.2f}\")\n"
      ],
      "metadata": {
        "id": "1B4v8cogerSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bayesian Optimization"
      ],
      "metadata": {
        "id": "0sP_4DAzlMFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the objective function for Optuna\n",
        "def objective(trial):\n",
        "    params = {\n",
        "        'booster': trial.suggest_categorical('booster', ['gbtree', 'dart']),\n",
        "        'objective': 'binary:logistic',\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
        "        'gamma': trial.suggest_float('gamma', 0, 0.5),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
        "        'min_child_weight': trial.suggest_float('min_child_weight', 0.5, 5),\n",
        "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 50, 300)\n",
        "    }\n",
        "\n",
        "    model = xgb.XGBClassifier(**params)\n",
        "\n",
        "    # Use cross-validation to evaluate the model\n",
        "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc')\n",
        "    return cv_scores.mean()\n",
        "\n",
        "# Create a study and optimize the objective function\n",
        "study = optuna.create_study(direction='maximize')\n",
        "\n",
        "\n",
        "# Enqueue the parameters obtained from RandomizedSearchCV\n",
        "study.enqueue_trial({\n",
        "    'booster': 'gbtree',\n",
        "    'objective': 'binary:logistic',\n",
        "    'learning_rate': 0.10666666666666666,\n",
        "    'gamma':  0.375,\n",
        "    'max_depth': 3,\n",
        "    'min_child_weight': 2,\n",
        "    'subsample': 0.5,\n",
        "    'colsample_bytree': 0.6111111111111112 ,\n",
        "    'n_estimators': 130\n",
        "})\n",
        "\n",
        "# Optimize the study\n",
        "study.optimize(objective, n_trials=50)\n",
        "\n",
        "# Print the best parameters and the best score\n",
        "print(f\"Best Parameters: {study.best_params}\")\n",
        "\n",
        "\n",
        "# Train the final model with the best parameters\n",
        "best_params = study.best_params\n",
        "final_model = xgb.XGBClassifier(**best_params)\n",
        "final_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = final_model.predict(X_test)\n",
        "\n",
        "# Evaluate the final model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Test Precision: {precision:.4f}\")\n",
        "print(f\"Test F1 Score: {f1:.4f}\")\n",
        "print(f\"Test Recall: {recall:.4f}\")\n"
      ],
      "metadata": {
        "id": "4MPocTsV9wjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_features = X_train.columns.tolist()\n",
        "\n",
        "best_params_xg = {\n",
        "    'learning_rate':   0.216735435271517,\n",
        "    'max_depth': 5,\n",
        "    'n_estimators': 118,\n",
        "    'gamma': 0.2985469943655731,\n",
        "    'min_child_weight': 0.5958575235613429,\n",
        "    'subsample': 0.5600864058115105,\n",
        "    'colsample_bytree': 0.737265031590481,\n",
        "    'objective': 'binary:logistic',\n",
        "    'booster': 'gbtree'\n",
        "}\n",
        "\n",
        "# Create the XGBoost classifier with the best parameters\n",
        "final_model_xg = xgb.XGBClassifier(**best_params_xg, random_state = 42)\n",
        "\n",
        "# Perform cross-validation\n",
        "cv_scores = cross_val_score(final_model_xg, X_train[selected_features], y_train, cv=5, scoring='roc_auc')\n",
        "print(\"Cross-validation scores:\")\n",
        "print(cv_scores)\n",
        "print(f\"Mean CV accuracy: {np.mean(cv_scores):.4f}\")\n",
        "\n",
        "# Train the model\n",
        "final_model_xg.fit(X_train[selected_features], y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_xg = final_model_xg.predict(X_test[selected_features])\n",
        "y_pred_prob_xg = final_model_xg.predict_proba(X_test[selected_features])[:, 1]\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred_xg)\n",
        "precision = precision_score(y_test, y_pred_xg, average=\"weighted\", zero_division=1)\n",
        "recall = recall_score(y_test, y_pred_xg)\n",
        "f1 = f1_score(y_test, y_pred_xg)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_prob_xg)\n",
        "\n",
        "# Calculate specificity\n",
        "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_xg).ravel()\n",
        "specificity = tn / (tn + fp)\n",
        "\n",
        "# Print the results\n",
        "print(f'Test ROC AUC: {roc_auc:.2f}')\n",
        "print(f'Test Accuracy: {accuracy:.2f}')\n",
        "print(f'Test Precision: {precision:.2f}')\n",
        "print(f'Test Recall: {recall:.2f}')\n",
        "print(f'Test F1-Score: {f1:.2f}')\n",
        "print(f'Test Specificity: {specificity:.2f}')\n"
      ],
      "metadata": {
        "id": "YXK3auzjAPWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##95% CI"
      ],
      "metadata": {
        "id": "kCgESUbAU4ok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the XGBoost classifier with the best parameters\n",
        "final_model_xg = xgb.XGBClassifier(**best_params_xg,random_state =42)\n",
        "\n",
        "# Perform cross-validation\n",
        "cv_scores = cross_val_score(final_model_xg, X_train[selected_features], y_train, cv=5, scoring='roc_auc')\n",
        "print(\"Cross-validation scores:\")\n",
        "print(cv_scores)\n",
        "print(f\"Mean CV accuracy: {np.mean(cv_scores):.4f}\")\n",
        "\n",
        "# Train the model\n",
        "final_model_xg.fit(X_train[selected_features], y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_xg = final_model_xg.predict(X_test[selected_features])\n",
        "y_pred_prob_xg = final_model_xg.predict_proba(X_test[selected_features])[:, 1]\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred_xg)\n",
        "precision = precision_score(y_test, y_pred_xg, average=\"weighted\", zero_division=1)\n",
        "recall = recall_score(y_test, y_pred_xg)\n",
        "f1 = f1_score(y_test, y_pred_xg)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_prob_xg)\n",
        "\n",
        "# Calculate specificity\n",
        "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_xg).ravel()\n",
        "specificity = tn / (tn + fp)\n",
        "\n",
        "# Bootstrap function to calculate 95% CI\n",
        "def bootstrap_metric(metric_func, y_true, y_pred, y_pred_prob=None, n_bootstraps=1000):\n",
        "    bootstrapped_scores = []\n",
        "    n_size = len(y_true)\n",
        "\n",
        "    for i in range(n_bootstraps):\n",
        "        # Resample with replacement\n",
        "        indices = resample(range(n_size), replace=True, n_samples=n_size)\n",
        "        y_true_resampled = np.array(y_true)[indices]\n",
        "        y_pred_resampled = np.array(y_pred)[indices]\n",
        "\n",
        "        if metric_func == roc_auc_score:\n",
        "            y_pred_prob_resampled = np.array(y_pred_prob)[indices]\n",
        "            score = metric_func(y_true_resampled, y_pred_prob_resampled)\n",
        "        elif metric_func in [accuracy_score, precision_score, recall_score, f1_score]:\n",
        "            score = metric_func(y_true_resampled, y_pred_resampled)\n",
        "        elif metric_func == \"specificity\":\n",
        "            tn, fp, fn, tp = confusion_matrix(y_true_resampled, y_pred_resampled).ravel()\n",
        "            score = tn / (tn + fp)\n",
        "\n",
        "        bootstrapped_scores.append(score)\n",
        "\n",
        "    # Calculate 95% CI\n",
        "    lower = np.percentile(bootstrapped_scores, 2.5)\n",
        "    upper = np.percentile(bootstrapped_scores, 97.5)\n",
        "\n",
        "    return lower, upper\n",
        "\n",
        "# Calculate 95% CI for each metric\n",
        "accuracy_ci = bootstrap_metric(accuracy_score, y_test, y_pred_xg)\n",
        "precision_ci = bootstrap_metric(precision_score, y_test, y_pred_xg)\n",
        "recall_ci = bootstrap_metric(recall_score, y_test, y_pred_xg)\n",
        "f1_ci = bootstrap_metric(f1_score, y_test, y_pred_xg)\n",
        "roc_auc_ci = bootstrap_metric(roc_auc_score, y_test, y_pred_xg, y_pred_prob=y_pred_prob_xg)\n",
        "specificity_ci = bootstrap_metric(\"specificity\", y_test, y_pred_xg)\n",
        "\n",
        "# Print the results with 95% CIs\n",
        "print(f'Test ROC AUC: {roc_auc:.2f} (95% CI: {roc_auc_ci[0]:.2f}, {roc_auc_ci[1]:.2f})')\n",
        "print(f'Test Accuracy: {accuracy:.2f} (95% CI: {accuracy_ci[0]:.2f}, {accuracy_ci[1]:.2f})')\n",
        "print(f'Test Precision: {precision:.2f} (95% CI: {precision_ci[0]:.2f}, {precision_ci[1]:.2f})')\n",
        "print(f'Test Recall: {recall:.2f} (95% CI: {recall_ci[0]:.2f}, {recall_ci[1]:.2f})')\n",
        "print(f'Test F1-Score: {f1:.2f} (95% CI: {f1_ci[0]:.2f}, {f1_ci[1]:.2f})')\n",
        "print(f'Test Specificity: {specificity:.2f} (95% CI: {specificity_ci[0]:.2f}, {specificity_ci[1]:.2f})')\n"
      ],
      "metadata": {
        "id": "FmDYFbiHIYSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Prediction on IBD"
      ],
      "metadata": {
        "id": "D9AbDE8n5nPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_val_ibd = ibd.drop('Group', axis=1)\n",
        "y_val_ibd = ibd['Group']\n",
        "\n",
        "# Encode the target variable\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded_ibd = label_encoder.fit_transform(y_val_ibd)\n",
        "\n",
        "# Identify missing features in the validation set\n",
        "missing_features = [feature for feature in selected_features if feature not in X_val_ibd.columns]\n",
        "\n",
        "# Add missing features with zero values\n",
        "missing_df = pd.DataFrame(0.0, index=X_val_ibd.index, columns=missing_features)\n",
        "X_val_ibd = pd.concat([X_val_ibd, missing_df], axis=1)\n",
        "\n",
        "# Ensure the columns are in the same order as the training features\n",
        "X_val_ibd = X_val_ibd[selected_features]\n",
        "xg_model.fit(X_val_ibd, y_encoded_ibd)\n",
        "\n",
        "# Get predicted probabilities and class predictions using model\n",
        "y_pred_prob_xg_ibd = xg_model.predict_proba(X_val_ibd)[:, 1]\n",
        "y_pred_xg_ibd = xg_model.predict(X_val_ibd)\n",
        "\n",
        "# Calculate ROC AUC and other metrics\n",
        "roc_auc_val_xg = roc_auc_score(y_encoded_ibd, y_pred_prob_xg_ibd)\n",
        "accuracy_val_xg = accuracy_score(y_encoded_ibd, y_pred_xg_ibd)\n",
        "precision_val_xg = precision_score(y_encoded_ibd, y_pred_xg_ibd, average=\"weighted\", zero_division=1)\n",
        "recall_val_xg = recall_score(y_encoded_ibd, y_pred_xg_ibd, average=\"weighted\")\n",
        "f1_val_xg = f1_score(y_encoded_ibd, y_pred_xg_ibd, average=\"weighted\")\n",
        "\n",
        "# Calculate specificity\n",
        "y_pred_raw_xg_ibd = final_model_xg.predict(X_val_ibd)\n",
        "conf_matrix_raw = confusion_matrix(y_encoded_ibd, y_pred_raw_xg_ibd)\n",
        "\n",
        "# Compute specificity\n",
        "if conf_matrix_raw.shape == (2, 2):\n",
        "    tn_val_xg, fp_val_xg, fn_val_xg, tp_val_xg = conf_matrix_raw.ravel()\n",
        "    specificity_val_xg = tn_val_xg / (tn_val_xg + fp_val_xg)\n",
        "else:\n",
        "    specificity_val_xg = 'N/A'\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(f'Validation ROC AUC: {roc_auc_val_xg:.2f}')\n",
        "print(f'Validation Accuracy: {accuracy_val_xg:.2f}')\n",
        "print(f'Validation Precision: {precision_val_xg:.2f}')\n",
        "print(f'Validation Recall: {recall_val_xg:.2f}')\n",
        "print(f'Validation F1-Score: {f1_val_xg:.2f}')\n",
        "print(f'Validation Specificity: {specificity_val_xg:.2f}')\n"
      ],
      "metadata": {
        "id": "5KOmE6lph2C2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##95% CI"
      ],
      "metadata": {
        "id": "fBDKriy_ZNjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bootstrap_confidence_interval(y_true, y_pred, y_pred_prob, metric_func, n_bootstraps=1000, alpha=0.95):\n",
        "    bootstrapped_scores = []\n",
        "    n_size = len(y_true)\n",
        "\n",
        "    for i in range(n_bootstraps):\n",
        "        # Sample with replacement from the data\n",
        "        indices = resample(np.arange(n_size), replace=True, n_samples=n_size)\n",
        "        if metric_func == roc_auc_score:\n",
        "            score = metric_func(y_true[indices], y_pred_prob[indices])\n",
        "        else:\n",
        "            score = metric_func(y_true[indices], y_pred[indices])\n",
        "        bootstrapped_scores.append(score)\n",
        "\n",
        "    # Calculate the confidence interval\n",
        "    sorted_scores = np.sort(bootstrapped_scores)\n",
        "    lower_bound = np.percentile(sorted_scores, (1 - alpha) / 2 * 100)\n",
        "    upper_bound = np.percentile(sorted_scores, (1 + alpha) / 2 * 100)\n",
        "\n",
        "    return lower_bound, upper_bound\n",
        "\n",
        "# Number of bootstrap samples and alpha for 95% CI\n",
        "n_bootstraps = 1000\n",
        "alpha = 0.95\n",
        "\n",
        "# Calculate 95% confidence intervals for the metrics\n",
        "roc_auc_ci = bootstrap_confidence_interval(y_encoded_ibd, y_pred_xg_ibd, y_pred_prob_xg_ibd, roc_auc_score, n_bootstraps, alpha)\n",
        "accuracy_ci = bootstrap_confidence_interval(y_encoded_ibd, y_pred_xg_ibd, y_pred_prob_xg_ibd, accuracy_score, n_bootstraps, alpha)\n",
        "precision_ci = bootstrap_confidence_interval(y_encoded_ibd, y_pred_xg_ibd, y_pred_prob_xg_ibd, precision_score, n_bootstraps, alpha)\n",
        "recall_ci = bootstrap_confidence_interval(y_encoded_ibd, y_pred_xg_ibd, y_pred_prob_xg_ibd, recall_score, n_bootstraps, alpha)\n",
        "f1_ci = bootstrap_confidence_interval(y_encoded_ibd, y_pred_xg_ibd, y_pred_prob_xg_ibd, f1_score, n_bootstraps, alpha)\n",
        "\n",
        "# Calculate specificity CI\n",
        "def calculate_specificity(y_true, y_pred):\n",
        "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "    if conf_matrix.shape == (2, 2):\n",
        "        tn, fp, fn, tp = conf_matrix.ravel()\n",
        "        return tn / (tn + fp)\n",
        "    else:\n",
        "        return None\n",
        "specificity_ci = bootstrap_confidence_interval(y_encoded_ibd, y_pred_xg_ibd, y_pred_prob_xg_ibd, calculate_specificity, n_bootstraps, alpha)\n",
        "\n",
        "# Print the metrics with 95% confidence intervals\n",
        "print(f'Validation ROC AUC: {roc_auc_val_xg:.2f} (95% CI: {roc_auc_ci[0]:.2f} - {roc_auc_ci[1]:.2f})')\n",
        "print(f'Validation Accuracy: {accuracy_val_xg:.2f} (95% CI: {accuracy_ci[0]:.2f} - {accuracy_ci[1]:.2f})')\n",
        "print(f'Validation Precision: {precision_val_xg:.2f} (95% CI: {precision_ci[0]:.2f} - {precision_ci[1]:.2f})')\n",
        "print(f'Validation Recall: {recall_val_xg:.2f} (95% CI: {recall_ci[0]:.2f} - {recall_ci[1]:.2f})')\n",
        "print(f'Validation F1-Score: {f1_val_xg:.2f} (95% CI: {f1_ci[0]:.2f} - {f1_ci[1]:.2f})')\n",
        "print(f'Validation Specificity: {specificity_val_xg:.2f} (95% CI: {specificity_ci[0]:.2f} - {specificity_ci[1]:.2f})')\n"
      ],
      "metadata": {
        "id": "XSkDuXRglfgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Prediction on CRC"
      ],
      "metadata": {
        "id": "ilLLivVY5u2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_val_cc = cc.drop('Group', axis=1)\n",
        "y_val_cc = cc['Group']\n",
        "\n",
        "# Encode the target variable\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded_cc = label_encoder.fit_transform(y_val_cc)\n",
        "\n",
        "# Identify missing features in the validation set\n",
        "missing_features = [feature for feature in selected_features if feature not in X_val_cc.columns]\n",
        "\n",
        "# Add missing features with zero values\n",
        "missing_df = pd.DataFrame(0.0, index=X_val_cc.index, columns=missing_features)\n",
        "X_val_cc = pd.concat([X_val_cc, missing_df], axis=1)\n",
        "\n",
        "# Ensure the columns are in the same order as the training features\n",
        "X_val_cc = X_val_cc[selected_features]\n",
        "xg_model.fit(X_val_cc, y_encoded_cc)\n",
        "\n",
        "# Get predicted probabilities and class predictions\n",
        "y_pred_prob_xg_cc = xg_model.predict_proba(X_val_cc)[:, 1]\n",
        "y_pred_xg_cc = xg_model.predict(X_val_cc)\n",
        "\n",
        "# Calculate ROC AUC and other metrics\n",
        "roc_auc_val_xg = roc_auc_score(y_encoded_cc, y_pred_prob_xg_cc)\n",
        "accuracy_val_xg = accuracy_score(y_encoded_cc, y_pred_xg_cc)\n",
        "precision_val_xg = precision_score(y_encoded_cc, y_pred_xg_cc, average=\"weighted\", zero_division=1)\n",
        "recall_val_xg = recall_score(y_encoded_cc, y_pred_xg_cc, average=\"weighted\")\n",
        "f1_val_xg = f1_score(y_encoded_cc, y_pred_xg_cc, average=\"weighted\")\n",
        "\n",
        "# Calculate specificity\n",
        "y_pred_raw_xg_cc = final_model_xg.predict(X_val_cc)\n",
        "conf_matrix_raw = confusion_matrix(y_encoded_cc, y_pred_raw_xg_cc)\n",
        "\n",
        "# Compute specificity\n",
        "if conf_matrix_raw.shape == (2, 2):\n",
        "    tn_val_xg, fp_val_xg, fn_val_xg, tp_val_xg = conf_matrix_raw.ravel()\n",
        "    specificity_val_xg = tn_val_xg / (tn_val_xg + fp_val_xg)\n",
        "else:\n",
        "    specificity_val_xg = 'N/A'\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(f'Validation ROC AUC: {roc_auc_val_xg:.2f}')\n",
        "print(f'Validation Accuracy: {accuracy_val_xg:.2f}')\n",
        "print(f'Validation Precision: {precision_val_xg:.2f}')\n",
        "print(f'Validation Recall: {recall_val_xg:.2f}')\n",
        "print(f'Validation F1-Score: {f1_val_xg:.2f}')\n",
        "print(f'Validation Specificity: {specificity_val_xg:.2f}')"
      ],
      "metadata": {
        "id": "c5osQa3k43hd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##95% CI"
      ],
      "metadata": {
        "id": "dO6FDS4OZUIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to calculate bootstrap confidence intervals\n",
        "def bootstrap_ci(y_true, y_pred, y_prob, metric_func, n_bootstrap=1000, ci=95):\n",
        "    scores = []\n",
        "    for _ in range(n_bootstrap):\n",
        "        # Resample data with replacement\n",
        "        indices = resample(range(len(y_true)), replace=True)\n",
        "        y_true_boot = y_true[indices]\n",
        "        y_pred_boot = y_pred[indices]\n",
        "        y_prob_boot = y_prob[indices]\n",
        "\n",
        "        # Calculate the metric on the bootstrap sample\n",
        "        score = metric_func(y_true_boot, y_pred_boot, y_prob_boot)\n",
        "        scores.append(score)\n",
        "\n",
        "    # Compute the confidence interval\n",
        "    lower_bound = np.percentile(scores, (100 - ci) / 2)\n",
        "    upper_bound = np.percentile(scores, 100 - (100 - ci) / 2)\n",
        "    return lower_bound, upper_bound\n",
        "\n",
        "# Define metric functions\n",
        "def roc_auc_func(y_true, y_pred, y_prob):\n",
        "    return roc_auc_score(y_true, y_prob)\n",
        "\n",
        "def accuracy_func(y_true, y_pred, y_prob):\n",
        "    return accuracy_score(y_true, y_pred)\n",
        "\n",
        "def precision_func(y_true, y_pred, y_prob):\n",
        "    return precision_score(y_true, y_pred, average=\"weighted\", zero_division=1)\n",
        "\n",
        "def recall_func(y_true, y_pred, y_prob):\n",
        "    return recall_score(y_true, y_pred, average=\"weighted\")\n",
        "\n",
        "def f1_func(y_true, y_pred, y_prob):\n",
        "    return f1_score(y_true, y_pred, average=\"weighted\")\n",
        "\n",
        "# Convert to numpy arrays for indexing\n",
        "y_true_array = np.array(y_encoded_cc)\n",
        "y_pred_array = np.array(y_pred_xg_cc)\n",
        "y_prob_array = np.array(y_pred_prob_xg_cc)\n",
        "\n",
        "# Calculate metrics with 95% CI\n",
        "roc_auc_ci = bootstrap_ci(y_true_array, y_pred_array, y_prob_array, roc_auc_func)\n",
        "accuracy_ci = bootstrap_ci(y_true_array, y_pred_array, y_prob_array, accuracy_func)\n",
        "precision_ci = bootstrap_ci(y_true_array, y_pred_array, y_prob_array, precision_func)\n",
        "recall_ci = bootstrap_ci(y_true_array, y_pred_array, y_prob_array, recall_func)\n",
        "f1_ci = bootstrap_ci(y_true_array, y_pred_array, y_prob_array, f1_func)\n",
        "specificity_ci = bootstrap_ci(y_true_array, y_pred_array, y_prob_array, lambda y_true, y_pred, y_prob: specificity_val_xg)\n",
        "\n",
        "# Print results with confidence intervals\n",
        "print(f'Validation ROC AUC: {roc_auc_val_xg:.2f} (95% CI: {roc_auc_ci[0]:.2f}, {roc_auc_ci[1]:.2f})')\n",
        "print(f'Validation Accuracy: {accuracy_val_xg:.2f} (95% CI: {accuracy_ci[0]:.2f}, {accuracy_ci[1]:.2f})')\n",
        "print(f'Validation Precision: {precision_val_xg:.2f} (95% CI: {precision_ci[0]:.2f}, {precision_ci[1]:.2f})')\n",
        "print(f'Validation Recall: {recall_val_xg:.2f} (95% CI: {recall_ci[0]:.2f}, {recall_ci[1]:.2f})')\n",
        "print(f'Validation F1-Score: {f1_val_xg:.2f} (95% CI: {f1_ci[0]:.2f}, {f1_ci[1]:.2f})')\n",
        "print(f'Validation Specificity: {specificity_val_xg:.2f} (95% CI: {specificity_ci[0]:.2f}, {specificity_ci[1]:.2f})')\n"
      ],
      "metadata": {
        "id": "Vhqlt7x75EC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Random Forest"
      ],
      "metadata": {
        "id": "YMtsKG5Y7OqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = gc.drop(['Group'], axis=1)\n",
        "y = gc['Group']\n",
        "\n",
        "# Encode categorical target labels into numerical labels\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Create the Random Forest classifier\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the classifier on the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy on the test set\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "# Calculate F1 score on the test set\n",
        "test_f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "print(f\"Test F1 Score: {test_f1:.4f}\")\n",
        "\n",
        "# Calculate precision on the test set\n",
        "test_precision = precision_score(y_test, y_pred, average='weighted')\n",
        "print(f\"Test Precision: {test_precision:.4f}\")\n",
        "\n",
        "# Calculate recall on the test set\n",
        "test_recall = recall_score(y_test, y_pred, average='weighted')\n",
        "print(f\"Test Recall: {test_recall:.4f}\")\n"
      ],
      "metadata": {
        "id": "dRrxt3yjzApY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Random Search"
      ],
      "metadata": {
        "id": "3SxbdbJ6uLQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Define the parameter grid for RandomizedSearchCV\n",
        "param_dist = {\n",
        "    'n_estimators': [int(x) for x in np.linspace(start=100, stop=1000, num=10)],\n",
        "    'max_features': ['sqrt', 'log2'],\n",
        "    'max_depth': [int(x) for x in np.linspace(10, 110, num=11)] + [None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "# Initialize the Random Forest model\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Initialize RandomizedSearchCV\n",
        "rf_random = RandomizedSearchCV(estimator=rf, param_distributions=param_dist,\n",
        "                               n_iter=100, cv=5, verbose=2, random_state=42, n_jobs=-1)\n",
        "\n",
        "# Fit RandomizedSearchCV to the data\n",
        "rf_random.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters found by RandomizedSearchCV\n",
        "print(\"Best parameters found by RandomizedSearchCV:\")\n",
        "print(rf_random.best_params_)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = rf_random.best_estimator_.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1}\")\n"
      ],
      "metadata": {
        "id": "ghDO8nqDzV5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bayesian Optimization"
      ],
      "metadata": {
        "id": "rl3hoe97uQA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Define the best parameters found from RandomizedSearchCV RF\n",
        "best_params_random = {\n",
        "    'n_estimators': 700,\n",
        "    'min_samples_split': 5,\n",
        "    'min_samples_leaf': 4,\n",
        "    'max_features': 'log2',\n",
        "    'max_depth': 80,\n",
        "    'bootstrap': True\n",
        "}\n",
        "\n",
        "def objective(trial):\n",
        "    # Define the search space based on the best parameters from RandomizedSearchCV\n",
        "    n_estimators = trial.suggest_int('n_estimators', max(100, best_params_random['n_estimators'] - 200), best_params_random['n_estimators'] + 200)\n",
        "    min_samples_split = trial.suggest_int('min_samples_split', max(2, best_params_random['min_samples_split'] - 3), best_params_random['min_samples_split'] + 3)\n",
        "    min_samples_leaf = trial.suggest_int('min_samples_leaf', max(1, best_params_random['min_samples_leaf'] - 2), best_params_random['min_samples_leaf'] + 2)\n",
        "    max_features = trial.suggest_categorical('max_features', ['sqrt', 'log2'])\n",
        "    max_depth = trial.suggest_int('max_depth', max(5, best_params_random['max_depth'] - 10), best_params_random['max_depth'] + 10)\n",
        "    bootstrap = trial.suggest_categorical('bootstrap', [True, False])\n",
        "\n",
        "    # Initialize RandomForestClassifier with suggested hyperparameters\n",
        "    clf = RandomForestClassifier(\n",
        "        n_estimators=n_estimators,\n",
        "        min_samples_split=min_samples_split,\n",
        "        min_samples_leaf=min_samples_leaf,\n",
        "        max_features=max_features,\n",
        "        max_depth=max_depth,\n",
        "        bootstrap=bootstrap,  # Fixed from best_params_random\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Use cross-validation to evaluate the classifier\n",
        "    cv_scores = cross_val_score(clf, X, y, cv=5, scoring='roc_auc')\n",
        "    return np.mean(cv_scores)\n",
        "\n",
        "# Perform optimization with Optuna\n",
        "study = optuna.create_study(direction='maximize')\n",
        "\n",
        "# Enqueue the trial with the best parameters from RandomizedSearchCV\n",
        "study.enqueue_trial(best_params_random)\n",
        "\n",
        "study.optimize(objective, n_trials=50)\n",
        "\n",
        "# Print the best parameters and best score from Optuna\n",
        "print(\"Best Parameters from Optuna:\", study.best_params)\n",
        "print(\"Best Accuracy from Optuna:\", study.best_value)\n",
        "\n",
        "# Retrieve the best model and evaluate on the test set\n",
        "best_params_optuna = study.best_params\n",
        "best_clf = RandomForestClassifier(**best_params_optuna, random_state=50)\n",
        "best_clf.fit(X_train, y_train)\n",
        "y_pred = best_clf.predict(X_test)\n",
        "\n",
        "# Evaluate on test set\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Test Precision: {precision:.4f}\")\n",
        "print(f\"Test Recall: {recall:.4f}\")\n",
        "print(f\"Test F1 Score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "567q9RAlzeIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "selected_features_rf = X_train.columns.tolist()\n",
        "\n",
        "# Best parameters from Bayesian Optimization RF\n",
        "best_params_rf = {\n",
        "    'n_estimators': 612,\n",
        "    'min_samples_split': 6,\n",
        "    'min_samples_leaf': 3,\n",
        "    'max_features': 'log2',\n",
        "    'max_depth': 84,\n",
        "    'bootstrap': False\n",
        "}\n",
        "\n",
        "\n",
        "# Create the Random Forest classifier with the best parameters\n",
        "final_model_rf = RandomForestClassifier(**best_params_rf, random_state=42)\n",
        "\n",
        "\n",
        "# Perform cross-validation with 5 folds\n",
        "cv_scores = cross_val_score(final_model_rf, X_train[selected_features_rf], y_train, cv=5, scoring='roc_auc')\n",
        "print(\"Cross-validation scores:\")\n",
        "print(cv_scores)\n",
        "print(f\"Mean CV accuracy: {np.mean(cv_scores):.4f}\")\n",
        "\n",
        "# Train the final model on the entire training data\n",
        "final_model_rf.fit(X_train[selected_features_rf], y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_rf = final_model_rf.predict(X_test[selected_features_rf])\n",
        "y_pred_prob_rf = final_model_rf.predict_proba(X_test[selected_features_rf])[:, 1]\n",
        "\n",
        "# Calculate evaluation metrics on the test set\n",
        "accuracy = accuracy_score(y_test, y_pred_rf)\n",
        "precision = precision_score(y_test, y_pred_rf, average='weighted')\n",
        "recall = recall_score(y_test, y_pred_rf, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred_rf, average='weighted')\n",
        "auc_roc = roc_auc_score(y_test, y_pred_prob_rf)\n",
        "\n",
        "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_rf).ravel()\n",
        "specificity = tn / (tn + fp)\n",
        "\n",
        "# Print the results for test data\n",
        "print(f'Test Accuracy: {accuracy:.2f}')\n",
        "print(f'Test Precision: {precision:.2f}')\n",
        "print(f'Test Recall: {recall:.2f}')\n",
        "print(f'Test F1-Score: {f1:.2f}')\n",
        "print(f'Test AUC-ROC: {auc_roc:.2f}')\n",
        "print(f'Test Specificity: {specificity:.2f}')\n"
      ],
      "metadata": {
        "id": "nK0PTIXQ2I9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##95% CI"
      ],
      "metadata": {
        "id": "hMfiNN2NZcNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bootstrap_metric(y_true, y_pred, y_pred_prob, n_iterations=1000, alpha=0.95):\n",
        "    metrics = {'accuracy': [],\n",
        "               'precision': [],\n",
        "               'recall': [],\n",
        "               'f1': [],\n",
        "               'roc_auc': [],\n",
        "               'specificity': []}\n",
        "\n",
        "    n_size = len(y_true)\n",
        "\n",
        "    for i in range(n_iterations):\n",
        "        # Bootstrap sample\n",
        "        indices = resample(np.arange(n_size), n_samples=n_size, replace=True)\n",
        "        y_true_boot = y_true[indices]\n",
        "        y_pred_boot = y_pred[indices]\n",
        "        y_pred_prob_boot = y_pred_prob[indices]\n",
        "\n",
        "        # Calculate metrics\n",
        "        accuracy = accuracy_score(y_true_boot, y_pred_boot)\n",
        "        precision = precision_score(y_true_boot, y_pred_boot)\n",
        "        recall = recall_score(y_true_boot, y_pred_boot)\n",
        "        f1 = f1_score(y_true_boot, y_pred_boot)\n",
        "        roc_auc = roc_auc_score(y_true_boot, y_pred_prob_boot)\n",
        "\n",
        "        tn, fp, fn, tp = confusion_matrix(y_true_boot, y_pred_boot).ravel()\n",
        "        specificity = tn / (tn + fp)\n",
        "\n",
        "        # Store results\n",
        "        metrics['accuracy'].append(accuracy)\n",
        "        metrics['precision'].append(precision)\n",
        "        metrics['recall'].append(recall)\n",
        "        metrics['f1'].append(f1)\n",
        "        metrics['roc_auc'].append(roc_auc)\n",
        "        metrics['specificity'].append(specificity)\n",
        "\n",
        "    # Calculate confidence intervals\n",
        "    ci = {}\n",
        "    for metric in metrics:\n",
        "        lower = np.percentile(metrics[metric], (1 - alpha) / 2 * 100)\n",
        "        upper = np.percentile(metrics[metric], (1 + alpha) / 2 * 100)\n",
        "        ci[metric] = (lower, upper)\n",
        "\n",
        "    return ci\n",
        "y_test = np.array(y_test)\n",
        "y_pred_rf = np.array(y_pred_rf)\n",
        "y_pred_prob_rf = np.array(y_pred_prob_rf)\n",
        "\n",
        "# Calculate metrics on the original data\n",
        "accuracy = accuracy_score(y_test, y_pred_rf)\n",
        "precision = precision_score(y_test, y_pred_rf)\n",
        "recall = recall_score(y_test, y_pred_rf)\n",
        "f1 = f1_score(y_test, y_pred_rf)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_prob_rf)\n",
        "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_rf).ravel()\n",
        "specificity = tn / (tn + fp)\n",
        "\n",
        "# Calculate confidence intervals\n",
        "ci = bootstrap_metric(y_test, y_pred_rf, y_pred_prob_rf, n_iterations=1000, alpha=0.95)\n",
        "\n",
        "# Print scores and confidence intervals\n",
        "print(f'Test ROC AUC: {roc_auc:.2f} (95% CI: {ci[\"roc_auc\"][0]:.2f}, {ci[\"roc_auc\"][1]:.2f})')\n",
        "print(f'Test Accuracy: {accuracy:.2f} (95% CI: {ci[\"accuracy\"][0]:.2f}, {ci[\"accuracy\"][1]:.2f})')\n",
        "print(f'Test Precision: {precision:.2f} (95% CI: {ci[\"precision\"][0]:.2f}, {ci[\"precision\"][1]:.2f})')\n",
        "print(f'Test Recall: {recall:.2f} (95% CI: {ci[\"recall\"][0]:.2f}, {ci[\"recall\"][1]:.2f})')\n",
        "print(f'Test F1-Score: {f1:.2f} (95% CI: {ci[\"f1\"][0]:.2f}, {ci[\"f1\"][1]:.2f})')\n",
        "print(f'Test Specificity: {specificity:.2f} (95% CI: {ci[\"specificity\"][0]:.2f}, {ci[\"specificity\"][1]:.2f})')"
      ],
      "metadata": {
        "id": "6KMtJPh63uSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Prediction on IBD"
      ],
      "metadata": {
        "id": "OP3MV8087_ml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_val_ibd_rf = ibd.drop('Group', axis=1)\n",
        "y_val_ibd_rf = ibd['Group']\n",
        "\n",
        "# Encode categorical target labels into numerical labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded_ibd_rf = label_encoder.fit_transform(y_val_ibd_rf)\n",
        "\n",
        "# Identify the features that are missing in the validation set\n",
        "missing_features = [feature for feature in selected_features_rf if feature not in X_val_ibd_rf.columns]\n",
        "\n",
        "# Add the missing features to the validation set with zero values using pd.concat\n",
        "missing_df_rf = pd.DataFrame(0, index=X_val_ibd_rf.index, columns=missing_features)\n",
        "X_val_ibd_rf = pd.concat([X_val_ibd_rf, missing_df_rf], axis=1)\n",
        "\n",
        "# Ensure the columns are in the same order as the training features\n",
        "X_val_ibd_rf = X_val_ibd_rf[selected_features_rf]\n",
        "rf_model.fit(X_val_ibd_rf, y_encoded_ibd_rf)\n",
        "\n",
        "# Get the predicted probabilities\n",
        "y_pred_prob_rf_ibd = rf_model.predict_proba(X_val_ibd_rf)[:, 1]\n",
        "\n",
        "# Predict the class labels\n",
        "y_pred_rf_ibd = rf_model.predict(X_val_ibd_rf)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "roc_auc_val_rf = roc_auc_score(y_encoded_ibd_rf, y_pred_prob_rf_ibd)\n",
        "\n",
        "# Calculate accuracy, precision, recall, f1-scores\n",
        "accuracy_val_rf = accuracy_score(y_encoded_ibd_rf, y_pred_rf_ibd)\n",
        "precision_val_rf = precision_score(y_encoded_ibd_rf, y_pred_rf_ibd, average=\"weighted\", zero_division=1)\n",
        "recall_val_rf = recall_score(y_encoded_ibd_rf, y_pred_rf_ibd, average=\"weighted\")\n",
        "f1_val_rf = f1_score(y_encoded_ibd_rf, y_pred_rf_ibd, average=\"weighted\")\n",
        "\n",
        "# Calculate confusion matrix\n",
        "conf_matrix = confusion_matrix(y_encoded_ibd_rf, y_pred_rf_ibd)\n",
        "\n",
        "# Ensure confusion matrix has the correct dimensions for TN, FP, FN, TP\n",
        "if conf_matrix.shape == (2, 2):\n",
        "    tn_val_rf, fp_val_rf, fn_val_rf, tp_val_rf = conf_matrix.ravel()\n",
        "    specificity_val_rf = tn_val_rf / (tn_val_rf + fp_val_rf)\n",
        "else:\n",
        "    specificity_val_rf = 'N/A'\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(f'Validation ROC AUC: {roc_auc_val_rf:.2f}')\n",
        "print(f'Validation Accuracy: {accuracy_val_rf:.2f}')\n",
        "print(f'Validation Precision: {precision_val_rf:.2f}')\n",
        "print(f'Validation Recall: {recall_val_rf:.2f}')\n",
        "print(f'Validation F1-Score: {f1_val_rf:.2f}')\n",
        "print(f'Validation Specificity: {specificity_val_rf:.2f}')\n"
      ],
      "metadata": {
        "id": "Iqjnkvo5047U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##95% CI"
      ],
      "metadata": {
        "id": "3d7gX2aIZmIh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bootstrap_confidence_interval(y_true, y_pred, y_pred_prob, metric_func, n_bootstraps=1000, alpha=0.95):\n",
        "    bootstrapped_scores = []\n",
        "    n_size = len(y_true)\n",
        "\n",
        "    # Perform bootstrapping\n",
        "    for i in range(n_bootstraps):\n",
        "        # Sample with replacement from the data\n",
        "        indices = resample(np.arange(n_size), replace=True, n_samples=n_size)\n",
        "        if metric_func == roc_auc_score:\n",
        "            score = metric_func(y_true[indices], y_pred_prob[indices])\n",
        "        else:\n",
        "            score = metric_func(y_true[indices], y_pred[indices])\n",
        "        bootstrapped_scores.append(score)\n",
        "\n",
        "    # Calculate confidence intervals\n",
        "    sorted_scores = np.sort(bootstrapped_scores)\n",
        "    lower_bound = np.percentile(sorted_scores, (1 - alpha) / 2 * 100)\n",
        "    upper_bound = np.percentile(sorted_scores, (1 + alpha) / 2 * 100)\n",
        "\n",
        "    return lower_bound, upper_bound\n",
        "\n",
        "# Number of bootstrap samples and alpha for 95% CI\n",
        "n_bootstraps = 1000\n",
        "alpha = 0.95\n",
        "\n",
        "# Calculate 95% confidence intervals for metrics\n",
        "roc_auc_ci = bootstrap_confidence_interval(y_encoded_ibd_rf, y_pred_rf_ibd, y_pred_prob_rf_ibd, roc_auc_score, n_bootstraps, alpha)\n",
        "accuracy_ci = bootstrap_confidence_interval(y_encoded_ibd_rf, y_pred_rf_ibd, y_pred_prob_rf_ibd, accuracy_score, n_bootstraps, alpha)\n",
        "precision_ci = bootstrap_confidence_interval(y_encoded_ibd_rf, y_pred_rf_ibd, y_pred_prob_rf_ibd, precision_score, n_bootstraps, alpha)\n",
        "recall_ci = bootstrap_confidence_interval(y_encoded_ibd_rf, y_pred_rf_ibd, y_pred_prob_rf_ibd, recall_score, n_bootstraps, alpha)\n",
        "f1_ci = bootstrap_confidence_interval(y_encoded_ibd_rf, y_pred_rf_ibd, y_pred_prob_rf_ibd, f1_score, n_bootstraps, alpha)\n",
        "\n",
        "# Specificity calculation\n",
        "def calculate_specificity(y_true, y_pred):\n",
        "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "    if conf_matrix.shape == (2, 2):\n",
        "        tn, fp, fn, tp = conf_matrix.ravel()\n",
        "        return tn / (tn + fp)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "specificity_ci = bootstrap_confidence_interval(y_encoded_ibd_rf, y_pred_rf_ibd, y_pred_prob_rf_ibd, calculate_specificity, n_bootstraps, alpha)\n",
        "\n",
        "# Print metrics with 95% confidence intervals\n",
        "print(f'Validation ROC AUC: {roc_auc_val_rf:.2f} (95% CI: {roc_auc_ci[0]:.2f} - {roc_auc_ci[1]:.2f})')\n",
        "print(f'Validation Accuracy: {accuracy_val_rf:.2f} (95% CI: {accuracy_ci[0]:.2f} - {accuracy_ci[1]:.2f})')\n",
        "print(f'Validation Precision: {precision_val_rf:.2f} (95% CI: {precision_ci[0]:.2f} - {precision_ci[1]:.2f})')\n",
        "print(f'Validation Recall: {recall_val_rf:.2f} (95% CI: {recall_ci[0]:.2f} - {recall_ci[1]:.2f})')\n",
        "print(f'Validation F1-Score: {f1_val_rf:.2f} (95% CI: {f1_ci[0]:.2f} - {f1_ci[1]:.2f})')\n",
        "print(f'Validation Specificity: {specificity_val_rf:.2f} (95% CI: {specificity_ci[0]:.2f} - {specificity_ci[1]:.2f})')\n"
      ],
      "metadata": {
        "id": "gQY_SzXj3YUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Predictions on CRC"
      ],
      "metadata": {
        "id": "cF-LfDYN8HfX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_val_cc_rf = cc.drop('Group', axis=1)\n",
        "y_val_cc_rf = cc['Group']\n",
        "\n",
        "# Encode categorical target labels into numerical labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded_cc_rf = label_encoder.fit_transform(y_val_cc_rf)\n",
        "\n",
        "# Identify and add missing features in the validation set\n",
        "missing_features = [feature for feature in selected_features_rf if feature not in X_val_cc_rf.columns]\n",
        "missing_df_rf = pd.DataFrame(0, index=X_val_cc_rf.index, columns=missing_features)\n",
        "X_val_cc_rf = pd.concat([X_val_cc_rf, missing_df_rf], axis=1)\n",
        "X_val_cc_rf = X_val_cc_rf[selected_features_rf]\n",
        "\n",
        "\n",
        "#Fit the model\n",
        "rf_model.fit(X_val_cc_rf, y_encoded_cc_rf)\n",
        "y_pred_prob_rf_cc = rf_model.predict_proba(X_val_cc_rf)[:, 1]\n",
        "y_pred_rf_cc = rf_model.predict(X_val_cc_rf)\n",
        "\n",
        "# Performance metrics\n",
        "roc_auc_val_rf = roc_auc_score(y_encoded_cc_rf, y_pred_prob_rf_cc)\n",
        "accuracy_val_rf = accuracy_score(y_encoded_cc_rf, y_pred_rf_cc)\n",
        "precision_val_rf = precision_score(y_encoded_cc_rf, y_pred_rf_cc, average=\"weighted\", zero_division=1)\n",
        "recall_val_rf = recall_score(y_encoded_cc_rf, y_pred_rf_cc, average=\"weighted\")\n",
        "f1_val_rf = f1_score(y_encoded_cc_rf, y_pred_rf_cc, average=\"weighted\")\n",
        "\n",
        "# Specificity\n",
        "conf_matrix = confusion_matrix(y_encoded_cc_rf, y_pred_rf_cc)\n",
        "if conf_matrix.shape == (2, 2):\n",
        "    tn_val_rf, fp_val_rf, fn_val_rf, tp_val_rf = conf_matrix.ravel()\n",
        "    specificity_val_rf = tn_val_rf / (tn_val_rf + fp_val_rf)\n",
        "else:\n",
        "    specificity_val_rf = 'N/A'\n",
        "\n",
        "# Print  evaluation metrics\n",
        "print(f'Validation ROC AUC: {roc_auc_val_rf:.2f}')\n",
        "print(f'Validation Accuracy: {accuracy_val_rf:.2f}')\n",
        "print(f'Validation Precision: {precision_val_rf:.2f}')\n",
        "print(f'Validation Recall: {recall_val_rf:.2f}')\n",
        "print(f'Validation F1-Score: {f1_val_rf:.2f}')\n",
        "print(f'Validation Specificity: {specificity_val_rf:.2f}')"
      ],
      "metadata": {
        "id": "o-tEZKZCsSXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##95% CI"
      ],
      "metadata": {
        "id": "p0OJPzZQZsAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_metrics(y_true, y_prob, y_pred):\n",
        "    roc_auc = roc_auc_score(y_true, y_prob)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred, average=\"weighted\", zero_division=1)\n",
        "    recall = recall_score(y_true, y_pred, average=\"weighted\")\n",
        "    f1 = f1_score(y_true, y_pred, average=\"weighted\")\n",
        "    return roc_auc, accuracy, precision, recall, f1\n",
        "\n",
        "# Bootstrapping for 95% CI\n",
        "n_bootstraps = 1000\n",
        "rng = np.random.default_rng(seed=42)\n",
        "\n",
        "metrics_bootstrapped = []\n",
        "for _ in range(n_bootstraps):\n",
        "    indices = rng.choice(range(len(y_encoded_cc_rf)), size=len(y_encoded_cc_rf), replace=True)\n",
        "    X_resampled = X_val_cc_rf.iloc[indices]\n",
        "    y_true_resampled = y_encoded_cc_rf[indices]\n",
        "\n",
        "    # Predictions on the bootstrap sample\n",
        "    y_prob_resampled = rf_model.predict_proba(X_resampled)[:, 1]\n",
        "    y_pred_resampled = rf_model.predict(X_resampled)\n",
        "\n",
        "    # Calculate metrics for the bootstrap sample\n",
        "    metrics_bootstrapped.append(calculate_metrics(y_true_resampled, y_prob_resampled, y_pred_resampled))\n",
        "\n",
        "# Convert bootstrapped metrics to NumPy array for easier calculations\n",
        "metrics_bootstrapped = np.array(metrics_bootstrapped)\n",
        "\n",
        "# Calculate 2.5th and 97.5th percentiles for each metric\n",
        "ci_lower = np.percentile(metrics_bootstrapped, 2.5, axis=0)\n",
        "ci_upper = np.percentile(metrics_bootstrapped, 97.5, axis=0)\n",
        "\n",
        "# Print metrics with 95% CIs\n",
        "print(f'Validation ROC AUC: {roc_auc_val_rf:.2f} (95% CI: {ci_lower[0]:.2f}-{ci_upper[0]:.2f})')\n",
        "print(f'Validation Accuracy: {accuracy_val_rf:.2f} (95% CI: {ci_lower[1]:.2f}-{ci_upper[1]:.2f})')\n",
        "print(f'Validation Precision: {precision_val_rf:.2f} (95% CI: {ci_lower[2]:.2f}-{ci_upper[2]:.2f})')\n",
        "print(f'Validation Recall: {recall_val_rf:.2f} (95% CI: {ci_lower[3]:.2f}-{ci_upper[3]:.2f})')\n",
        "print(f'Validation F1-Score: {f1_val_rf:.2f} (95% CI: {ci_lower[4]:.2f}-{ci_upper[4]:.2f})')\n",
        "\n",
        "# Function to calculate specificity\n",
        "def calculate_specificity(y_true, y_pred):\n",
        "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "    if conf_matrix.shape == (2, 2):\n",
        "        tn, fp, fn, tp = conf_matrix.ravel()\n",
        "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "    else:\n",
        "        specificity = 'N/A'\n",
        "    return specificity\n",
        "\n",
        "# Bootstrapping for 95% CI of specificity\n",
        "specificity_bootstrapped = []\n",
        "for _ in range(n_bootstraps):\n",
        "    # Resample the validation dataset with replacement\n",
        "    indices = rng.choice(range(len(y_encoded_cc_rf)), size=len(y_encoded_cc_rf), replace=True)\n",
        "    X_resampled = X_val_cc_rf.iloc[indices]\n",
        "    y_true_resampled = y_encoded_cc_rf[indices]\n",
        "\n",
        "    # Predictions on the bootstrap sample\n",
        "    y_pred_resampled = rf_model.predict(X_resampled)\n",
        "\n",
        "    # Calculate specificity for the bootstrap sample\n",
        "    specificity_bootstrapped.append(calculate_specificity(y_true_resampled, y_pred_resampled))\n",
        "\n",
        "# Convert to NumPy array for percentile calculations\n",
        "specificity_bootstrapped = np.array(specificity_bootstrapped)\n",
        "\n",
        "# Calculate 2.5th and 97.5th percentiles for specificity\n",
        "specificity_ci_lower = np.percentile(specificity_bootstrapped, 2.5)\n",
        "specificity_ci_upper = np.percentile(specificity_bootstrapped, 97.5)\n",
        "\n",
        "# Print specificity with 95% CI\n",
        "print(f'Validation Specificity: {specificity_val_rf:.2f} (95% CI: {specificity_ci_lower:.2f}-{specificity_ci_upper:.2f})')"
      ],
      "metadata": {
        "id": "6uWQ4RkY2dKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LASSO"
      ],
      "metadata": {
        "id": "kXd7xO1N9zHt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = gc.drop(['Group'], axis=1)\n",
        "y = gc['Group']\n",
        "\n",
        "# Encode categorical target labels into numerical labels\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Create Logistic Regression classifier with L1 regularization\n",
        "log_reg = LogisticRegression(penalty='l1', solver='liblinear', random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = log_reg.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'F1 Score: {f1:.4f}')\n"
      ],
      "metadata": {
        "id": "0H9B4TOWC5Fp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Random Search"
      ],
      "metadata": {
        "id": "aQcP5m7Xv_rA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Define the parameter grid for Randomized Search\n",
        "param_dist = {\n",
        "    'C': np.logspace(-4, 4, 20),\n",
        "    'solver': ['liblinear'],\n",
        "    'max_iter': [1000, 1500, 2000],\n",
        "    'tol': [1e-4, 1e-3, 1e-2, 1e-1]\n",
        "}\n",
        "\n",
        "# Create Logistic Regression classifier with L1 regularization (Lasso)\n",
        "log_reg = LogisticRegression(penalty='l1', random_state=42)\n",
        "\n",
        "# Set up the Randomized Search with cross-validation\n",
        "random_search = RandomizedSearchCV(\n",
        "    log_reg, param_distributions=param_dist, n_iter=100,\n",
        "    scoring='roc_auc', cv=5, verbose=1, random_state=42, n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit the Randomized Search model\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "best_params = random_search.best_params_\n",
        "print(\"Best Parameters:\", best_params)\n",
        "\n",
        "# Predict on the test set with the best model\n",
        "y_pred = random_search.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'F1 Score: {f1:.4f}')\n",
        "\n"
      ],
      "metadata": {
        "id": "mbsnX0zjDD6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bayesian Optimization"
      ],
      "metadata": {
        "id": "XBWhP6AwwLvo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Define the best parameters found from previous searches lasso\n",
        "best_params_random = {\n",
        "    'C': 78.47599703514607,\n",
        "    'max_iter': 1500,\n",
        "    'tol': 0.0001,\n",
        "    'solver': 'liblinear'\n",
        "}\n",
        "\n",
        "def objective(trial):\n",
        "    C = trial.suggest_float('C', 1e-4, 1e2, log=True)\n",
        "    max_iter = trial.suggest_int('max_iter', 2000, 5000)\n",
        "    tol = trial.suggest_float('tol', 1e-4, 1e-2, log=True)\n",
        "    solver = trial.suggest_categorical('solver', ['liblinear', 'saga'])\n",
        "\n",
        "    # Initialize LogisticRegression with hyperparameters\n",
        "    clf = LogisticRegression(\n",
        "        penalty='l1',\n",
        "        C=C,\n",
        "        max_iter=max_iter,\n",
        "        tol=tol,\n",
        "        solver=solver,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Use cross-validation to evaluate the classifier\n",
        "    score = cross_val_score(clf, X_train, y_train, cv=5, scoring='roc_auc').mean()\n",
        "    return score\n",
        "\n",
        "# Perform optimization with Optuna\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=42)\n",
        "\n",
        "# Print the best parameters and best score from Optuna\n",
        "print(\"Best Parameters from Optuna:\", study.best_params)\n",
        "print(\"Best Accuracy from Optuna:\", study.best_value)\n",
        "\n",
        "# Retrieve the best model and evaluate on the test set\n",
        "best_params_optuna = study.best_params\n",
        "best_clf = LogisticRegression(penalty='l1', **best_params_optuna, random_state=42)\n",
        "best_clf.fit(X_train, y_train)\n",
        "y_pred = best_clf.predict(X_test)\n",
        "\n",
        "# Evaluate on test set\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Test Precision: {precision:.4f}\")\n",
        "print(f\"Test Recall: {recall:.4f}\")\n",
        "print(f\"Test F1 Score: {f1:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "wfCc9NO8Crb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_features_lasso = X_train.columns.tolist()\n",
        "\n",
        "# Best parameters from Bayesian Optimization with Optuna\n",
        "best_params_lasso = {\n",
        "    'C':  14.007388155394787,\n",
        "    'max_iter': 3223,\n",
        "    'solver': 'saga',\n",
        "    'tol': 0.0008450953267578447\n",
        "}\n",
        "\n",
        "# Train the Logistic Regression model again using only the selected features\n",
        "final_model_lasso = LogisticRegression(penalty='l1', **best_params_lasso, random_state=42)\n",
        "\n",
        "# Perform cross-validation with 5 folds\n",
        "cv_scores = cross_val_score(final_model_lasso, X_train[selected_features_lasso], y_train, cv=5, scoring='roc_auc')\n",
        "print(\"Cross-validation scores:\")\n",
        "print(cv_scores)\n",
        "print(f\"Mean CV accuracy: {np.mean(cv_scores):.4f}\")\n",
        "\n",
        "final_model_lasso.fit(X_train[selected_features_lasso], y_train)\n",
        "\n",
        "# Make predictions on the test set (predicted probabilities)\n",
        "y_pred_proba_lasso = final_model_lasso.predict_proba(X_test[selected_features_lasso])[:, 1]\n",
        "\n",
        "# Convert probabilities to predicted class labels\n",
        "y_pred_lasso = final_model_lasso.predict(X_test[selected_features_lasso])\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred_lasso)\n",
        "precision = precision_score(y_test, y_pred_lasso)\n",
        "recall = recall_score(y_test, y_pred_lasso)\n",
        "f1 = f1_score(y_test, y_pred_lasso)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba_lasso)\n",
        "\n",
        "# Calculate confusion matrix and extract TN, FP, FN, TP\n",
        "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_lasso).ravel()\n",
        "\n",
        "# Calculate specificity\n",
        "specificity = tn / (tn + fp)\n",
        "\n",
        "print(f'Test ROC AUC: {roc_auc:.2f}')\n",
        "print(f'Test Accuracy: {accuracy:.2f}')\n",
        "print(f'Test Precision: {precision:.2f}')\n",
        "print(f'Test Recall: {recall:.2f}')\n",
        "print(f'Test F1-Score: {f1:.2f}')\n",
        "print(f'Test Specificity: {specificity:.2f}')"
      ],
      "metadata": {
        "id": "3S-V2hakEfVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##95% CI"
      ],
      "metadata": {
        "id": "3wPJBvGzZ1Sf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of bootstrap samples\n",
        "n_bootstraps = 1000\n",
        "rng = np.random.RandomState(42)\n",
        "\n",
        "# Initialize lists to store bootstrap results\n",
        "accuracy_bootstrap = []\n",
        "precision_bootstrap = []\n",
        "recall_bootstrap = []\n",
        "f1_bootstrap = []\n",
        "specificity_bootstrap = []\n",
        "roc_auc_bootstrap = []\n",
        "\n",
        "# Perform bootstrapping\n",
        "for i in range(n_bootstraps):\n",
        "    # Resample the dataset with replacement\n",
        "    indices = resample(np.arange(len(y_test)), random_state=rng)\n",
        "    y_true_resampled = y_test[indices]\n",
        "    X_test_resampled = X_test.iloc[indices]\n",
        "    y_pred_resampled = final_model_lasso.predict(X_test_resampled[selected_features_lasso])\n",
        "    y_pred_proba_resampled = final_model_lasso.predict_proba(X_test_resampled[selected_features_lasso])[:, 1]\n",
        "\n",
        "    # Calculate metrics on the resampled data\n",
        "    accuracy_bootstrap.append(accuracy_score(y_true_resampled, y_pred_resampled))\n",
        "    precision_bootstrap.append(precision_score(y_true_resampled, y_pred_resampled, zero_division=1))\n",
        "    recall_bootstrap.append(recall_score(y_true_resampled, y_pred_resampled, zero_division=1))\n",
        "    f1_bootstrap.append(f1_score(y_true_resampled, y_pred_resampled, zero_division=1))\n",
        "    roc_auc_bootstrap.append(roc_auc_score(y_true_resampled, y_pred_proba_resampled))\n",
        "\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true_resampled, y_pred_resampled).ravel()\n",
        "    specificity_bootstrap.append(tn / (tn + fp))\n",
        "\n",
        "# Calculate 95% confidence intervals\n",
        "def ci(metric_values):\n",
        "    return np.percentile(metric_values, [2.5, 97.5])\n",
        "\n",
        "accuracy_ci = ci(accuracy_bootstrap)\n",
        "precision_ci = ci(precision_bootstrap)\n",
        "recall_ci = ci(recall_bootstrap)\n",
        "f1_ci = ci(f1_bootstrap)\n",
        "specificity_ci = ci(specificity_bootstrap)\n",
        "roc_auc_ci = ci(roc_auc_bootstrap)\n",
        "\n",
        "# Print evaluation metrics and their 95% confidence intervals\n",
        "print(f'Test ROC AUC: {roc_auc:.2f} (95% CI: {roc_auc_ci[0]:.2f}, {roc_auc_ci[1]:.2f})')\n",
        "print(f'Test Accuracy: {accuracy:.2f} (95% CI: {accuracy_ci[0]:.2f}, {accuracy_ci[1]:.2f})')\n",
        "print(f'Test Precision: {precision:.2f} (95% CI: {precision_ci[0]:.2f}, {precision_ci[1]:.2f})')\n",
        "print(f'Test Recall: {recall:.2f} (95% CI: {recall_ci[0]:.2f}, {recall_ci[1]:.2f})')\n",
        "print(f'Test F1-Score: {f1:.2f} (95% CI: {f1_ci[0]:.2f}, {f1_ci[1]:.2f})')\n",
        "print(f'Test Specificity: {specificity:.2f} (95% CI: {specificity_ci[0]:.2f}, {specificity_ci[1]:.2f})')\n"
      ],
      "metadata": {
        "id": "IPn_Who9FVZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Predictions on IBD"
      ],
      "metadata": {
        "id": "ekbOXuiYgHtE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_val_ibd_lasso = ibd.drop('Group', axis=1)\n",
        "y_val_ibd_lasso = ibd['Group']\n",
        "\n",
        "# Encode categorical target labels into numerical labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded_ibd_lasso = label_encoder.fit_transform(y_val_ibd_lasso)\n",
        "\n",
        "# Identify the features that are missing in the validation set\n",
        "missing_features = [feature for feature in selected_features_lasso if feature not in X_val_ibd_lasso.columns]\n",
        "\n",
        "# Add the missing features to the validation set with zero values using pd.concat\n",
        "missing_df_lasso = pd.DataFrame(0, index=X_val_ibd_lasso.index, columns=missing_features)\n",
        "X_val_ibd_lasso = pd.concat([X_val_ibd_lasso, missing_df_lasso], axis=1)\n",
        "\n",
        "# Ensure the columns are in the same order as the training features\n",
        "X_val_ibd_lasso = X_val_ibd_lasso[selected_features_lasso]\n",
        "lasso_model.fit(X_val_ibd_lasso, y_encoded_ibd_lasso)\n",
        "\n",
        "# Get the predicted probabilities\n",
        "y_pred_prob_lasso_ibd = lasso_model.predict_proba(X_val_ibd_lasso)[:, 1]\n",
        "\n",
        "# Predict the class labels\n",
        "y_pred_lasso_ibd = lasso_model.predict(X_val_ibd_lasso)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "roc_auc_val_lasso = roc_auc_score(y_encoded_ibd_lasso, y_pred_prob_lasso_ibd)\n",
        "\n",
        "# Calculate accuracy, precision, recall, f1-score\n",
        "accuracy_val_lasso = accuracy_score(y_encoded_ibd_lasso, y_pred_lasso_ibd)\n",
        "precision_val_lasso = precision_score(y_encoded_ibd_lasso, y_pred_lasso_ibd, average=\"weighted\", zero_division=1)\n",
        "recall_val_lasso = recall_score(y_encoded_ibd_lasso, y_pred_lasso_ibd, average=\"weighted\")\n",
        "f1_val_lasso = f1_score(y_encoded_ibd_lasso, y_pred_lasso_ibd, average=\"weighted\")\n",
        "\n",
        "# Calculate confusion matrix\n",
        "conf_matrix = confusion_matrix(y_encoded_ibd_lasso, y_pred_lasso_ibd)\n",
        "\n",
        "# Ensure confusion matrix has the correct dimensions for TN, FP, FN, TP\n",
        "if conf_matrix.shape == (2, 2):\n",
        "    tn_val_lasso, fp_val_lasso, fn_val_lasso, tp_val_lasso = conf_matrix.ravel()\n",
        "    specificity_val_lasso = tn_val_lasso / (tn_val_lasso + fp_val_lasso)\n",
        "else:\n",
        "    specificity_val_lasso = 'N/A'\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(f'Validation ROC AUC: {roc_auc_val_lasso:.2f}')\n",
        "print(f'Validation Accuracy: {accuracy_val_lasso:.2f}')\n",
        "print(f'Validation Precision: {precision_val_lasso:.2f}')\n",
        "print(f'Validation Recall: {recall_val_lasso:.2f}')\n",
        "print(f'Validation F1-Score: {f1_val_lasso:.2f}')\n",
        "print(f'Validation Specificity: {specificity_val_lasso:.2f}')\n"
      ],
      "metadata": {
        "id": "lnO71jLjADpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##95% CI"
      ],
      "metadata": {
        "id": "LIJKESO_Z6HN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def bootstrap_confidence_interval(y_true, y_pred, y_pred_prob, metric_func, n_bootstraps=1000, alpha=0.95):\n",
        "    bootstrapped_scores = []\n",
        "    n_size = len(y_true)\n",
        "\n",
        "    # Perform bootstrapping\n",
        "    for i in range(n_bootstraps):\n",
        "        # Sample with replacement from the data\n",
        "        indices = resample(np.arange(n_size), replace=True, n_samples=n_size)\n",
        "        if metric_func == roc_auc_score:\n",
        "            score = metric_func(y_true[indices], y_pred_prob[indices])\n",
        "        else:\n",
        "            score = metric_func(y_true[indices], y_pred[indices])\n",
        "        bootstrapped_scores.append(score)\n",
        "\n",
        "    # Calculate confidence intervals\n",
        "    sorted_scores = np.sort(bootstrapped_scores)\n",
        "    lower_bound = np.percentile(sorted_scores, (1 - alpha) / 2 * 100)\n",
        "    upper_bound = np.percentile(sorted_scores, (1 + alpha) / 2 * 100)\n",
        "\n",
        "    return lower_bound, upper_bound\n",
        "\n",
        "# Number of bootstrap samples and alpha for 95% CI\n",
        "n_bootstraps = 1000\n",
        "alpha = 0.95\n",
        "\n",
        "# Calculate 95% confidence intervals for metrics\n",
        "roc_auc_ci = bootstrap_confidence_interval(y_encoded_ibd_lasso, y_pred_lasso_ibd, y_pred_prob_lasso_ibd, roc_auc_score, n_bootstraps, alpha)\n",
        "accuracy_ci = bootstrap_confidence_interval(y_encoded_ibd_lasso, y_pred_lasso_ibd, y_pred_prob_lasso_ibd, accuracy_score, n_bootstraps, alpha)\n",
        "precision_ci = bootstrap_confidence_interval(y_encoded_ibd_lasso, y_pred_lasso_ibd, y_pred_prob_lasso_ibd, precision_score, n_bootstraps, alpha)\n",
        "recall_ci = bootstrap_confidence_interval(y_encoded_ibd_lasso, y_pred_lasso_ibd, y_pred_prob_lasso_ibd, recall_score, n_bootstraps, alpha)\n",
        "f1_ci = bootstrap_confidence_interval(y_encoded_ibd_lasso, y_pred_lasso_ibd, y_pred_prob_lasso_ibd, f1_score, n_bootstraps, alpha)\n",
        "\n",
        "# For specificity, handle separately based on the confusion matrix\n",
        "def calculate_specificity(y_true, y_pred):\n",
        "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "    if conf_matrix.shape == (2, 2):\n",
        "        tn, fp, fn, tp = conf_matrix.ravel()\n",
        "        return tn / (tn + fp)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "specificity_ci = bootstrap_confidence_interval(y_encoded_ibd_lasso, y_pred_lasso_ibd, y_pred_prob_lasso_ibd, calculate_specificity, n_bootstraps, alpha)\n",
        "\n",
        "# Print metrics with 95% confidence intervals\n",
        "print(f'Validation ROC AUC: {roc_auc_val_lasso:.2f} (95% CI: {roc_auc_ci[0]:.2f} - {roc_auc_ci[1]:.2f})')\n",
        "print(f'Validation Accuracy: {accuracy_val_lasso:.2f} (95% CI: {accuracy_ci[0]:.2f} - {accuracy_ci[1]:.2f})')\n",
        "print(f'Validation Precision: {precision_val_lasso:.2f} (95% CI: {precision_ci[0]:.2f} - {precision_ci[1]:.2f})')\n",
        "print(f'Validation Recall: {recall_val_lasso:.2f} (95% CI: {recall_ci[0]:.2f} - {recall_ci[1]:.2f})')\n",
        "print(f'Validation F1-Score: {f1_val_lasso:.2f} (95% CI: {f1_ci[0]:.2f} - {f1_ci[1]:.2f})')\n",
        "print(f'Validation Specificity: {specificity_val_lasso:.2f} (95% CI: {specificity_ci[0]:.2f} - {specificity_ci[1]:.2f})')\n"
      ],
      "metadata": {
        "id": "dPdBHao6Bp2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Predictions on CRC"
      ],
      "metadata": {
        "id": "OMz3snfqgKUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_val_cc_lasso = cc.drop('Group', axis=1)\n",
        "y_val_cc_lasso = cc['Group']\n",
        "\n",
        "# Encode categorical target labels into numerical labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded_cc_lasso = label_encoder.fit_transform(y_val_cc_lasso)\n",
        "\n",
        "# Identify the features that are missing in the validation set\n",
        "missing_features = [feature for feature in selected_features_lasso if feature not in X_val_cc_lasso.columns]\n",
        "\n",
        "# Add the missing features to the validation set with zero values using pd.concat\n",
        "missing_df_lasso = pd.DataFrame(0, index=X_val_cc_lasso.index, columns=missing_features)\n",
        "X_val_cc_lasso = pd.concat([X_val_cc_lasso, missing_df_lasso], axis=1)\n",
        "\n",
        "# Ensure the columns are in the same order as the training features\n",
        "X_val_cc_lasso = X_val_cc_lasso[selected_features_lasso]\n",
        "lasso_model.fit(X_val_cc_lasso, y_encoded_cc_lasso)\n",
        "\n",
        "# Get the predicted probabilities\n",
        "y_pred_prob_lasso_cc = lasso_model.predict_proba(X_val_cc_lasso)[:, 1]\n",
        "\n",
        "# Predict the class labels\n",
        "y_pred_lasso_cc = lasso_model.predict(X_val_cc_lasso)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "roc_auc_val_lasso = roc_auc_score(y_encoded_cc_lasso, y_pred_prob_lasso_cc)\n",
        "\n",
        "# Calculate accuracy, precision, recall, f1-score\n",
        "accuracy_val_lasso = accuracy_score(y_encoded_cc_lasso, y_pred_lasso_cc)\n",
        "precision_val_lasso = precision_score(y_encoded_cc_lasso, y_pred_lasso_cc, average=\"weighted\", zero_division=1)\n",
        "recall_val_lasso = recall_score(y_encoded_cc_lasso, y_pred_lasso_cc, average=\"weighted\")\n",
        "f1_val_lasso = f1_score(y_encoded_cc_lasso, y_pred_lasso_cc, average=\"weighted\")\n",
        "\n",
        "# Calculate confusion matrix\n",
        "conf_matrix = confusion_matrix(y_encoded_cc_lasso, y_pred_lasso_cc)\n",
        "\n",
        "# Ensure confusion matrix has the correct dimensions for TN, FP, FN, TP\n",
        "if conf_matrix.shape == (2, 2):\n",
        "    tn_val_lasso, fp_val_lasso, fn_val_lasso, tp_val_lasso = conf_matrix.ravel()\n",
        "    specificity_val_lasso = tn_val_lasso / (tn_val_lasso + fp_val_lasso)\n",
        "else:\n",
        "    specificity_val_lasso = 'N/A'\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(f'Validation ROC AUC: {roc_auc_val_lasso:.2f}')\n",
        "print(f'Validation Accuracy: {accuracy_val_lasso:.2f}')\n",
        "print(f'Validation Precision: {precision_val_lasso:.2f}')\n",
        "print(f'Validation Recall: {recall_val_lasso:.2f}')\n",
        "print(f'Validation F1-Score: {f1_val_lasso:.2f}')\n",
        "print(f'Validation Specificity: {specificity_val_lasso:.2f}')"
      ],
      "metadata": {
        "id": "IHjDxEYb997K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##95% CI"
      ],
      "metadata": {
        "id": "n_sncqnnZ-yA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bootstrap_confidence_interval(y_true, y_pred, y_pred_prob, metric_func, n_bootstraps=1000, alpha=0.95):\n",
        "    bootstrapped_scores = []\n",
        "    n_size = len(y_true)\n",
        "\n",
        "    # Perform bootstrapping\n",
        "    for i in range(n_bootstraps):\n",
        "        indices = resample(np.arange(n_size), replace=True, n_samples=n_size)\n",
        "        if metric_func == roc_auc_score:\n",
        "            score = metric_func(y_true[indices], y_pred_prob[indices])\n",
        "        else:\n",
        "            score = metric_func(y_true[indices], y_pred[indices])\n",
        "        bootstrapped_scores.append(score)\n",
        "\n",
        "    # Calculate confidence intervals\n",
        "    sorted_scores = np.sort(bootstrapped_scores)\n",
        "    lower_bound = np.percentile(sorted_scores, (1 - alpha) / 2 * 100)\n",
        "    upper_bound = np.percentile(sorted_scores, (1 + alpha) / 2 * 100)\n",
        "\n",
        "    return lower_bound, upper_bound\n",
        "\n",
        "# Number of bootstrap samples and alpha for 95% CI\n",
        "n_bootstraps = 1000\n",
        "alpha = 0.95\n",
        "\n",
        "# Calculate 95% confidence intervals for metrics\n",
        "roc_auc_ci = bootstrap_confidence_interval(y_encoded_cc_lasso, y_pred_lasso_cc, y_pred_prob_lasso_cc, roc_auc_score, n_bootstraps, alpha)\n",
        "accuracy_ci = bootstrap_confidence_interval(y_encoded_cc_lasso, y_pred_lasso_cc, y_pred_prob_lasso_cc, accuracy_score, n_bootstraps, alpha)\n",
        "precision_ci = bootstrap_confidence_interval(y_encoded_cc_lasso, y_pred_lasso_cc, y_pred_prob_lasso_cc, precision_score, n_bootstraps, alpha)\n",
        "recall_ci = bootstrap_confidence_interval(y_encoded_cc_lasso, y_pred_lasso_cc, y_pred_prob_lasso_cc, recall_score, n_bootstraps, alpha)\n",
        "f1_ci = bootstrap_confidence_interval(y_encoded_cc_lasso, y_pred_lasso_cc, y_pred_prob_lasso_cc, f1_score, n_bootstraps, alpha)\n",
        "\n",
        "# For specificity, handle separately based on the confusion matrix\n",
        "def calculate_specificity(y_true, y_pred):\n",
        "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "    if conf_matrix.shape == (2, 2):\n",
        "        tn, fp, fn, tp = conf_matrix.ravel()\n",
        "        return tn / (tn + fp)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "specificity_ci = bootstrap_confidence_interval(y_encoded_cc_lasso, y_pred_lasso_cc, y_pred_prob_lasso_cc, calculate_specificity, n_bootstraps, alpha)\n",
        "\n",
        "# Print metrics with 95% confidence intervals\n",
        "print(f'Validation ROC AUC: {roc_auc_val_lasso:.2f} (95% CI: {roc_auc_ci[0]:.2f} - {roc_auc_ci[1]:.2f})')\n",
        "print(f'Validation Accuracy: {accuracy_val_lasso:.2f} (95% CI: {accuracy_ci[0]:.2f} - {accuracy_ci[1]:.2f})')\n",
        "print(f'Validation Precision: {precision_val_lasso:.2f} (95% CI: {precision_ci[0]:.2f} - {precision_ci[1]:.2f})')\n",
        "print(f'Validation Recall: {recall_val_lasso:.2f} (95% CI: {recall_ci[0]:.2f} - {recall_ci[1]:.2f})')\n",
        "print(f'Validation F1-Score: {f1_val_lasso:.2f} (95% CI: {f1_ci[0]:.2f} - {f1_ci[1]:.2f})')\n",
        "print(f'Validation Specificity: {specificity_val_lasso:.2f} (95% CI: {specificity_ci[0]:.2f} - {specificity_ci[1]:.2f})')\n"
      ],
      "metadata": {
        "id": "djO-3nIDBOFZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}