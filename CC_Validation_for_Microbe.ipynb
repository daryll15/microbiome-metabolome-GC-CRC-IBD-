{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###Import necessary libraries"
      ],
      "metadata": {
        "id": "_H6W9nPiziog"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "gkzWMBdWPndb"
      },
      "outputs": [],
      "source": [
        "! pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TD-V3kRPTdU1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import os,os.path\n",
        "import re\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "import optuna\n",
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.utils import resample\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from scipy.stats import bootstrap"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "import xgboost\n",
        "\n",
        "print(f\"Scikit-learn version: {sklearn.__version__}\")\n",
        "print(f\"XGBoost version: {xgboost.__version__}\")"
      ],
      "metadata": {
        "id": "pxJXNDa6ieiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall scikit-learn -y\n",
        "!pip install scikit-learn==1.5.2"
      ],
      "metadata": {
        "id": "SeEoNPXOKjCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Load the datasets"
      ],
      "metadata": {
        "id": "Om5Tl0oGetyv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gene = pd.read_excel(\"CC microbes.xlsx\")\n",
        "gene"
      ],
      "metadata": {
        "id": "gvCzemvQ9pqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gene_cc = pd.read_excel(\"CC- microbe Validation Kim .xlsx\")\n",
        "gene_cc"
      ],
      "metadata": {
        "id": "ZCXlTHEf_mpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_column = gene_cc['Group']\n",
        "data_to_scale = gene_cc.drop(columns=['Group'])\n",
        "\n",
        "# Apply Min-Max scaling to the data (excluding the 'Group' column)\n",
        "scaler = MinMaxScaler()\n",
        "data_scaled = scaler.fit_transform(data_to_scale)\n",
        "\n",
        "# Convert the scaled data back into a DataFrame and reattach the 'Group' column\n",
        "gene_cc = pd.DataFrame(data_scaled, columns=data_to_scale.columns)\n",
        "gene_cc['Group'] = sample_column\n",
        "\n",
        "# Rearrange the 'Sample' column as the first column\n",
        "gene_cc = gene_cc[['Group'] + [col for col in gene_cc.columns if col != 'Group']]\n",
        "\n",
        "\n",
        "print(\"Data after Min-Max scaling:\")\n",
        "print(gene_cc)"
      ],
      "metadata": {
        "id": "3DCguWl8_3sd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#XGBOOST"
      ],
      "metadata": {
        "id": "nJHgc2Q0ALNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = gene.drop(['Group'], axis=1)\n",
        "y = gene['Group']\n",
        "\n",
        "# Encode categorical target labels into numerical labels\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split data into training and testing sets (25% test, 75% train)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Initialize XGBoost classifier\n",
        "model = xgb.XGBClassifier(eval_metric='logloss', random_state=42)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the original test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "print(f'Test Precision: {precision:.4f}')\n",
        "print(f'Test F1 Score: {f1:.4f}')\n",
        "print(f'Test Recall: {recall:.4f}')"
      ],
      "metadata": {
        "id": "m0Rz1UA0Yxi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Random Search"
      ],
      "metadata": {
        "id": "AMMNXpnpUPP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Define the parameter grid for RandomizedSearchCV\n",
        "param_dist = {\n",
        "    'n_estimators': np.arange(50, 200, 10),\n",
        "    'max_depth': np.arange(3, 10),\n",
        "    'learning_rate': np.linspace(0.01, 0.3, 10),\n",
        "    'subsample': np.linspace(0.5, 1.0, 10),\n",
        "    'colsample_bytree': np.linspace(0.5, 1.0, 10),\n",
        "    'gamma': np.linspace(0, 0.5, 5),\n",
        "    'min_child_weight': np.arange(1, 6)\n",
        "}\n",
        "\n",
        "# Initialize the XGBoost classifier\n",
        "xgb = XGBClassifier(random_state=42)\n",
        "\n",
        "# Set up RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=xgb, param_distributions=param_dist, n_iter=100,\n",
        "    scoring='roc_auc', cv=5, verbose=1, random_state=42, n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit the RandomizedSearchCV model on the training data\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters from the random search\n",
        "best_params = random_search.best_params_\n",
        "print(\"Best Parameters:\", best_params)\n",
        "\n",
        "# Predict on the original test set\n",
        "y_pred = random_search.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Print metrics\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1 Score: {f1:.2f}\")"
      ],
      "metadata": {
        "id": "yBLm1dEwZ1un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bayesian Optimization"
      ],
      "metadata": {
        "id": "P3XyDsJZU0Ai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the objective function for Optuna\n",
        "def objective(trial):\n",
        "    params = {\n",
        "        'booster': trial.suggest_categorical('booster', ['gbtree', 'dart']),\n",
        "        'objective': 'binary:logistic',\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
        "        'gamma': trial.suggest_float('gamma', 0, 0.5),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
        "        'min_child_weight': trial.suggest_float('min_child_weight', 0.5, 5),\n",
        "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 50, 300)\n",
        "    }\n",
        "\n",
        "    # Initialize the XGBoost model with the suggested hyperparameters\n",
        "    model = xgb.XGBClassifier(**params)\n",
        "\n",
        "    # Evaluate using cross-validation\n",
        "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc')\n",
        "    return cv_scores.mean()\n",
        "\n",
        "# Create a study to maximize accuracy\n",
        "study = optuna.create_study(direction='maximize')\n",
        "\n",
        "\n",
        "# Enqueue the parameters obtained from previous RandomizedSearchCV results\n",
        "study.enqueue_trial({\n",
        "    'booster': 'gbtree',\n",
        "    'objective': 'binary:logistic',\n",
        "    'learning_rate':0.10666666666666666,\n",
        "    'gamma':0.0,\n",
        "    'max_depth': 3,\n",
        "    'min_child_weight': 3,\n",
        "    'subsample': 0.5555555555555556,\n",
        "    'colsample_bytree':0.6666666666666666,\n",
        "    'n_estimators': 90\n",
        "})\n",
        "\n",
        "# Optimize the study using 50 trials\n",
        "study.optimize(objective, n_trials=50)\n",
        "\n",
        "# Print the best parameters and cross-validation accuracy\n",
        "print(f\"Best Parameters: {study.best_params}\")\n",
        "print(f\"Best Cross-validation Accuracy: {study.best_value:.4f}\")\n",
        "\n",
        "# Train the final model with the best parameters on the training data\n",
        "best_params = study.best_params\n",
        "final_model = xgb.XGBClassifier(**best_params)\n",
        "final_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the original test set\n",
        "y_pred = final_model.predict(X_test)\n",
        "\n",
        "# Evaluate the final model on the test set\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Print test set performance metrics\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Test Precision: {precision:.4f}\")\n",
        "print(f\"Test F1 Score: {f1:.4f}\")\n",
        "print(f\"Test Recall: {recall:.4f}\")"
      ],
      "metadata": {
        "id": "Z6VeeeDEaW2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_features = X_train.columns.tolist()\n",
        "\n",
        "# Best parameters from Bayesian Optimization XGBoost\n",
        "best_params_xg = {\n",
        "    'learning_rate':0.10348781744369619,\n",
        "    'max_depth': 8,\n",
        "    'n_estimators': 245,\n",
        "    'gamma': 0.30657520470925953,\n",
        "    'min_child_weight': 2.3744771713372073,\n",
        "    'subsample': 0.5257213400166539,\n",
        "    'colsample_bytree':0.6599710995094004,\n",
        "    'objective': 'binary:logistic',\n",
        "    'booster': 'dart',\n",
        "}\n",
        "\n",
        "# Create the XGBoost classifier with the best parameters\n",
        "final_model_xg = xgb.XGBClassifier(**best_params_xg)\n",
        "\n",
        "# Perform cross-validation\n",
        "cv_scores = cross_val_score(final_model_xg, X_train[selected_features], y_train, cv=5, scoring='roc_auc')\n",
        "print(\"Cross-validation scores:\")\n",
        "print(cv_scores)\n",
        "print(f\"Mean CV accuracy: {np.mean(cv_scores):.4f}\")\n",
        "\n",
        "# Train the model\n",
        "final_model_xg.fit(X_train[selected_features], y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_xg = final_model_xg.predict(X_test[selected_features])\n",
        "y_pred_prob_xg = final_model_xg.predict_proba(X_test[selected_features])[:, 1]\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred_xg)\n",
        "precision = precision_score(y_test, y_pred_xg, average = 'weighted')\n",
        "recall = recall_score(y_test, y_pred_xg, average = 'weighted')\n",
        "f1 = f1_score(y_test, y_pred_xg, average = 'weighted')\n",
        "roc_auc = roc_auc_score(y_test, y_pred_prob_xg)\n",
        "\n",
        "# Calculate specificity\n",
        "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_xg).ravel()\n",
        "specificity = tn / (tn + fp)\n",
        "\n",
        "# Print the results\n",
        "print(f'Test ROC AUC: {roc_auc:.2f}')\n",
        "print(f'Test Accuracy: {accuracy:.2f}')\n",
        "print(f'Test Precision: {precision:.2f}')\n",
        "print(f'Test Recall: {recall:.2f}')\n",
        "print(f'Test F1-Score: {f1:.2f}')\n",
        "print(f'Test Specificity: {specificity:.2f}')\n"
      ],
      "metadata": {
        "id": "9Qq3Hin9d2iB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##XGBoost Validation"
      ],
      "metadata": {
        "id": "MHvcv0nt9kFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_val_cc = gene_cc.drop('Group', axis=1)\n",
        "y_val_cc = gene_cc['Group']\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y_val_cc)\n",
        "\n",
        "# Assuming you have the feature names used in training saved\n",
        "missing_features = [feature for feature in selected_features if feature not in X_val_cc.columns]\n",
        "\n",
        "# Add the missing features to the validation set with zero values\n",
        "missing_df = pd.DataFrame(0.0, index=X_val_cc.index, columns=missing_features)\n",
        "X_val_cc = pd.concat([X_val_cc, missing_df], axis=1)\n",
        "\n",
        "# Ensure the columns are in the same order as the training features\n",
        "X_val_cc = X_val_cc[selected_features]\n",
        "\n",
        "#Make predictions on the validation set\n",
        "y_pred_prob_xg_cc= final_model_xg.predict_proba(X_val_cc)[:, 1]\n",
        "y_pred_xg_cc = final_model_xg.predict(X_val_cc)\n",
        "\n",
        "# Use the encoded labels for all metrics\n",
        "y_val_cc_encoded = label_encoder.transform(y_val_cc)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy_val = accuracy_score(y_val_cc_encoded, y_pred_xg_cc)\n",
        "precision_val = precision_score(y_val_cc_encoded, y_pred_xg_cc, average=\"weighted\", zero_division=1)\n",
        "recall_val = recall_score(y_val_cc_encoded, y_pred_xg_cc, average=\"weighted\")\n",
        "f1_val = f1_score(y_val_cc_encoded, y_pred_xg_cc, average=\"weighted\")\n",
        "roc_auc_val = roc_auc_score(y_encoded, y_pred_prob_xg_cc)\n",
        "\n",
        "# Calculate specificity\n",
        "tn_val, fp_val, fn_val, tp_val = confusion_matrix(y_val_cc_encoded, y_pred_xg_cc).ravel()\n",
        "specificity_val = tn_val / (tn_val + fp_val)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(f'Validation ROC AUC: {roc_auc_val:.2f}')\n",
        "print(f'Validation Accuracy: {accuracy_val:.2f}')\n",
        "print(f'Validation Precision: {precision_val:.2f}')\n",
        "print(f'Validation Recall: {recall_val:.2f}')\n",
        "print(f'Validation F1-Score: {f1_val:.2f}')\n",
        "print(f'Validation Specificity: {specificity_val:.2f}')\n"
      ],
      "metadata": {
        "id": "ehIRr-bM5uLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##95% CI"
      ],
      "metadata": {
        "id": "UIpn0S8sROaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate the 95% confidence interval using bootstrapping\n",
        "def calculate_confidence_interval(metrics, confidence_level=0.95):\n",
        "    lower = np.percentile(metrics, (1 - confidence_level) / 2 * 100)\n",
        "    upper = np.percentile(metrics, (1 + confidence_level) / 2 * 100)\n",
        "    return lower, upper\n",
        "\n",
        "# Function to calculate evaluation metrics\n",
        "def calculate_metrics(y_true, y_pred, y_prob):\n",
        "    accuracy_val = accuracy_score(y_true, y_pred)\n",
        "    precision_val = precision_score(y_true, y_pred, average=\"weighted\", zero_division=1)\n",
        "    recall_val = recall_score(y_true, y_pred, average=\"weighted\")\n",
        "    f1_val = f1_score(y_true, y_pred, average=\"weighted\")\n",
        "    roc_auc_val = roc_auc_score(y_true, y_prob)\n",
        "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "    tn_val, fp_val, fn_val, tp_val = conf_matrix.ravel()\n",
        "    specificity_val = tn_val / (tn_val + fp_val)\n",
        "\n",
        "    return accuracy_val, precision_val, recall_val, f1_val, roc_auc_val, specificity_val\n",
        "\n",
        "# Bootstrapping to calculate the 95% confidence intervals\n",
        "n_iterations = 1000\n",
        "metrics_list = []\n",
        "\n",
        "for _ in range(n_iterations):\n",
        "    # Bootstrap resampling\n",
        "    resample_indices = np.random.choice(len(X_val_cc), size=len(X_val_cc), replace=True)\n",
        "    X_resample = X_val_cc.iloc[resample_indices]\n",
        "    y_resample = y_encoded[resample_indices]\n",
        "\n",
        "    # Get predictions for the resampled data\n",
        "    y_pred_prob_resample = final_model_xg.predict_proba(X_resample)[:, 1]\n",
        "    y_pred_resample = final_model_xg.predict(X_resample)\n",
        "\n",
        "    # Calculate metrics for the resampled data\n",
        "    metrics = calculate_metrics(y_resample, y_pred_resample, y_pred_prob_resample)\n",
        "    metrics_list.append(metrics)\n",
        "\n",
        "# Convert list of metrics to numpy array for CI calculation\n",
        "metrics_array = np.array(metrics_list)\n",
        "\n",
        "# Calculate 95% confidence intervals for each metric\n",
        "accuracy_ci = calculate_confidence_interval(metrics_array[:, 0])\n",
        "precision_ci = calculate_confidence_interval(metrics_array[:, 1])\n",
        "recall_ci = calculate_confidence_interval(metrics_array[:, 2])\n",
        "f1_ci = calculate_confidence_interval(metrics_array[:, 3])\n",
        "roc_auc_ci = calculate_confidence_interval(metrics_array[:, 4])\n",
        "specificity_ci = calculate_confidence_interval(metrics_array[:, 5])\n",
        "\n",
        "# Print evaluation metrics with confidence intervals\n",
        "print(f'Validation ROC AUC: {roc_auc_ci[0]:.2f} - {roc_auc_ci[1]:.2f} (95% CI)')\n",
        "print(f'Validation Accuracy: {accuracy_ci[0]:.2f} - {accuracy_ci[1]:.2f} (95% CI)')\n",
        "print(f'Validation Precision: {precision_ci[0]:.2f} - {precision_ci[1]:.2f} (95% CI)')\n",
        "print(f'Validation Recall: {recall_ci[0]:.2f} - {recall_ci[1]:.2f} (95% CI)')\n",
        "print(f'Validation F1-Score: {f1_ci[0]:.2f} - {f1_ci[1]:.2f} (95% CI)')\n",
        "print(f'Validation Specificity: {specificity_ci[0]:.2f} - {specificity_ci[1]:.2f} (95% CI)')\n",
        "\n"
      ],
      "metadata": {
        "id": "8_YqyfKTJQFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Random Forest"
      ],
      "metadata": {
        "id": "gjI8m9_Z3B7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = gene.drop(['Group'], axis=1)\n",
        "y = gene['Group']\n",
        "\n",
        "# Encode categorical target labels into numerical labels\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split data into training and testing sets (25% test, 75% train)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Create the Random Forest classifier\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the classifier on the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy on the test set\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n",
        "\n",
        "# Calculate F1 score on the test set\n",
        "test_f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "print(f\"Test F1 Score: {test_f1:.2f}\")\n",
        "\n",
        "# Calculate precision on the test set\n",
        "test_precision = precision_score(y_test, y_pred, average='weighted')\n",
        "print(f\"Test Precision: {test_precision:.2f}\")\n",
        "\n",
        "# Calculate recall on the test set\n",
        "test_recall = recall_score(y_test, y_pred, average='weighted')\n",
        "print(f\"Test Recall: {test_recall:.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "QTcF3TU4JuE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Random Search"
      ],
      "metadata": {
        "id": "fjIuEoYnKPLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Define the parameter distribution for RandomizedSearchCV\n",
        "param_dist = {\n",
        "    'rf__n_estimators': [int(x) for x in np.linspace(start=200, stop=2000, num=10)],\n",
        "    'rf__max_features': ['sqrt', 'log2'],\n",
        "    'rf__max_depth': [int(x) for x in np.linspace(10, 300, num=20)] + [None],\n",
        "    'rf__min_samples_split': [2, 5, 10, 15],\n",
        "    'rf__min_samples_leaf': [1, 2, 4, 6],\n",
        "    'rf__bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "# Initialize the pipeline:Random Forest\n",
        "pipeline = Pipeline([\n",
        "    ('rf', RandomForestClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "# Initialize RandomizedSearchCV\n",
        "rf_random = RandomizedSearchCV(estimator=pipeline, param_distributions=param_dist,\n",
        "                               n_iter=100, cv=StratifiedKFold(5), verbose=2,\n",
        "                               random_state=42, n_jobs=-1, scoring='roc_auc')\n",
        "\n",
        "# Fit RandomizedSearchCV to the original training data\n",
        "rf_random.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters found by RandomizedSearchCV\n",
        "print(\"Best parameters found by RandomizedSearchCV:\")\n",
        "print(rf_random.best_params_)\n",
        "\n",
        "# Predict on the original test data\n",
        "y_pred = rf_random.best_estimator_.predict(X_test)\n",
        "y_prob = rf_random.best_estimator_.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluate the model with default threshold (0.5)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"ROC AUC: {roc_auc:.4f}\")\n"
      ],
      "metadata": {
        "id": "_g076no6KOAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bayesian Optimization"
      ],
      "metadata": {
        "id": "NV9B0tvfihJT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the best parameters found from RandomizedSearchCV RF\n",
        "best_params_random = {\n",
        "    'n_estimators': 1400,\n",
        "    'min_samples_split': 5,\n",
        "    'min_samples_leaf':2,\n",
        "    'max_features': 'sqrt',\n",
        "    'max_depth': 55,\n",
        "    'bootstrap': True,\n",
        "    'class_weight': 'balanced'\n",
        "}\n",
        "\n",
        "def objective(trial):\n",
        "    n_estimators = trial.suggest_int('n_estimators', max(100, best_params_random['n_estimators'] - 200), best_params_random['n_estimators'] + 200)\n",
        "    min_samples_split = trial.suggest_int('min_samples_split', max(2, best_params_random['min_samples_split'] - 3), best_params_random['min_samples_split'] + 3)\n",
        "    min_samples_leaf = trial.suggest_int('min_samples_leaf', max(1, best_params_random['min_samples_leaf'] - 2), best_params_random['min_samples_leaf'] + 2)\n",
        "    max_features = trial.suggest_categorical('max_features', ['sqrt', 'log2'])\n",
        "    max_depth = trial.suggest_categorical('max_depth', [None, 10, 55, 162, 300])\n",
        "    bootstrap = trial.suggest_categorical('bootstrap', [True, False])\n",
        "    class_weight = trial.suggest_categorical('class_weight', ['balanced', None])\n",
        "    criterion = trial.suggest_categorical('criterion', ['gini', 'entropy'])\n",
        "    min_impurity_decrease = trial.suggest_float('min_impurity_decrease', 0.0, 0.01)\n",
        "\n",
        "    # Initialize RandomForestClassifier with suggested hyperparameters\n",
        "    clf = RandomForestClassifier(\n",
        "        n_estimators=n_estimators,\n",
        "        min_samples_split=min_samples_split,\n",
        "        min_samples_leaf=min_samples_leaf,\n",
        "        max_features=max_features,\n",
        "        max_depth=max_depth,\n",
        "        bootstrap=bootstrap,\n",
        "        class_weight=class_weight,\n",
        "        criterion=criterion,\n",
        "        min_impurity_decrease=min_impurity_decrease,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Use cross-validation to evaluate the classifier\n",
        "    cv_scores = cross_val_score(clf, X_train, y_train, cv=5, scoring='roc_auc')\n",
        "    return np.mean(cv_scores)\n",
        "\n",
        "# Perform optimization with Optuna\n",
        "study = optuna.create_study(direction='maximize')\n",
        "\n",
        "# Enqueue the trial with the best parameters from RandomizedSearchCV\n",
        "study.enqueue_trial(best_params_random)\n",
        "\n",
        "study.optimize(objective, n_trials=50)\n",
        "\n",
        "# Print the best parameters and best score from Optuna\n",
        "print(\"Best Parameters from Optuna:\", study.best_params)\n",
        "\n",
        "# Retrieve the best model and evaluate on the test set\n",
        "best_params_optuna = study.best_params\n",
        "best_clf = RandomForestClassifier(**best_params_optuna, random_state=50)\n",
        "best_clf.fit(X_train, y_train)\n",
        "y_pred = best_clf.predict(X_test)\n",
        "y_prob = best_clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluate on test set\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Test Precision: {precision:.4f}\")\n",
        "print(f\"Test Recall: {recall:.4f}\")\n",
        "print(f\"Test F1 Score: {f1:.4f}\")\n",
        "print(f\"Test ROC AUC: {roc_auc:.4f}\")\n"
      ],
      "metadata": {
        "id": "YZzMDqz-BpDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_features_rf = X_train.columns.tolist()\n",
        "\n",
        "# Best parameters from Bayesian Optimization RF\n",
        "best_params_rf = {\n",
        "    'n_estimators': 1495,\n",
        "    'min_samples_split': 2,\n",
        "    'min_samples_leaf': 1,\n",
        "    'max_features': 'log2',\n",
        "    'bootstrap': False,\n",
        "    'criterion': 'entropy',\n",
        "    'max_depth': 300,\n",
        "    'min_impurity_decrease':0.009209613291116275\n",
        "}\n",
        "\n",
        "## Create the Random Forest classifier with the best parameters\n",
        "final_model_rf = RandomForestClassifier(**best_params_rf, random_state=50)\n",
        "\n",
        "# Perform cross-validation with 5 folds\n",
        "cv_scores = cross_val_score(final_model_rf, X_train[selected_features_rf], y_train, cv=5, scoring='roc_auc')\n",
        "print(\"Cross-validation scores:\")\n",
        "print(cv_scores)\n",
        "print(f\"Mean CV accuracy: {np.mean(cv_scores):.4f}\")\n",
        "\n",
        "# Train the final model on the entire training data\n",
        "final_model_rf.fit(X_train[selected_features_rf], y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_rf = final_model_rf.predict(X_test[selected_features_rf])\n",
        "y_pred_prob_rf = final_model_rf.predict_proba(X_test[selected_features_rf])[:, 1]\n",
        "\n",
        "# Calculate evaluation metrics on the test set\n",
        "accuracy = accuracy_score(y_test, y_pred_rf)\n",
        "precision = precision_score(y_test, y_pred_rf, average = 'weighted')\n",
        "recall = recall_score(y_test, y_pred_rf, average = 'weighted')\n",
        "f1 = f1_score(y_test, y_pred_rf, average = 'weighted')\n",
        "auc_roc = roc_auc_score(y_test, y_pred_prob_rf)\n",
        "\n",
        "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_rf).ravel()\n",
        "specificity = tn / (tn + fp)\n",
        "\n",
        "# Print the results for test data\n",
        "print(f'Test Accuracy: {accuracy:.2f}')\n",
        "print(f'Test Precision: {precision:.2f}')\n",
        "print(f'Test Recall: {recall:.2f}')\n",
        "print(f'Test F1-Score: {f1:.2f}')\n",
        "print(f'Test AUC-ROC: {auc_roc:.2f}')\n",
        "print(f'Test Specificity: {specificity:.2f}')\n"
      ],
      "metadata": {
        "id": "OQyePtN0RtGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Random Forest Validation"
      ],
      "metadata": {
        "id": "Ic-LqGHLabpi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation set\n",
        "X_val_cc_rf = gene_cc.drop('Group', axis=1)\n",
        "y_val_cc_rf = gene_cc['Group']\n",
        "\n",
        "# Encode categorical target labels into numerical labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y_val_cc_rf)\n",
        "\n",
        "# Identify the features that are missing in the validation set\n",
        "missing_features = [feature for feature in selected_features_rf if feature not in X_val_cc_rf.columns]\n",
        "\n",
        "# Add the missing features to the validation set with zero values\n",
        "missing_features = [feature for feature in selected_features_rf if feature not in X_val_cc_rf.columns]\n",
        "missing_df_rf = pd.DataFrame(0, index=X_val_cc_rf.index, columns=missing_features)\n",
        "X_val_cc_rf = pd.concat([X_val_cc_rf, missing_df_rf], axis=1)\n",
        "\n",
        "# Reorder columns in X_val_cc_rf to match selected_features_rf\n",
        "X_val_cc_rf = X_val_cc_rf[selected_features_rf]\n",
        "\n",
        "# Make predictions on the validation set\n",
        "y_pred_prob_rf_cc = final_model_rf.predict_proba(X_val_cc_rf)[:, 1]\n",
        "y_pred_rf_cc = final_model_rf.predict(X_val_cc_rf)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy_val = accuracy_score(y_encoded, y_pred_rf_cc)\n",
        "precision_val = precision_score(y_encoded, y_pred_rf_cc, average='weighted', zero_division=1)\n",
        "recall_val = recall_score(y_encoded, y_pred_rf_cc, average='weighted')\n",
        "f1_val = f1_score(y_encoded, y_pred_rf_cc, average='weighted')\n",
        "roc_auc_val = roc_auc_score(y_encoded, y_pred_prob_rf_cc)\n",
        "\n",
        "# Calculate specificity\n",
        "tn_val, fp_val, fn_val, tp_val = confusion_matrix(y_encoded, y_pred_rf_cc).ravel()\n",
        "specificity_val = tn_val / (tn_val + fp_val)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(f'Validation ROC AUC: {roc_auc_val:.2f}')\n",
        "print(f'Validation Accuracy: {accuracy_val:.2f}')\n",
        "print(f'Validation Precision: {precision_val:.2f}')\n",
        "print(f'Validation Recall: {recall_val:.2f}')\n",
        "print(f'Validation F1-Score: {f1_val:.2f}')\n",
        "print(f'Validation Specificity: {specificity_val:.2f}')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DEVuh8LnxUS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##95% CI"
      ],
      "metadata": {
        "id": "Umn74ygaa_9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute metrics\n",
        "def compute_metrics(y_true, y_pred, y_pred_proba):\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred, average='weighted', zero_division=1)\n",
        "    recall = recall_score(y_true, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "    roc_auc = roc_auc_score(y_true, y_pred_proba)\n",
        "\n",
        "    # Confusion matrix to compute specificity\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    specificity = tn / (tn + fp)\n",
        "\n",
        "    return accuracy, precision, recall, f1, roc_auc, specificity\n",
        "\n",
        "# Number of bootstrap iterations\n",
        "n_iterations = 1000\n",
        "n_size = len(X_val_cc_rf)\n",
        "\n",
        "# Initialize lists to store metric values for each bootstrap sample\n",
        "accuracy_scores = []\n",
        "precision_scores = []\n",
        "recall_scores = []\n",
        "f1_scores = []\n",
        "roc_auc_scores = []\n",
        "specificity_scores = []\n",
        "\n",
        "# Bootstrap procedure\n",
        "for i in range(n_iterations):\n",
        "    # Resample the validation set with replacement\n",
        "    X_resample, y_resample = resample(X_val_cc_rf, y_encoded, n_samples=n_size, random_state=i)\n",
        "\n",
        "    # Make predictions on the resampled data\n",
        "    y_pred_resample = final_model_rf.predict(X_resample)\n",
        "    y_pred_proba_resample = final_model_rf.predict_proba(X_resample)[:, 1]\n",
        "\n",
        "    # Calculate metrics for this bootstrap sample\n",
        "    accuracy, precision, recall, f1, roc_auc, specificity = compute_metrics(y_resample, y_pred_resample, y_pred_proba_resample)\n",
        "\n",
        "    # Store the metrics\n",
        "    accuracy_scores.append(accuracy)\n",
        "    precision_scores.append(precision)\n",
        "    recall_scores.append(recall)\n",
        "    f1_scores.append(f1)\n",
        "    roc_auc_scores.append(roc_auc)\n",
        "    specificity_scores.append(specificity)\n",
        "\n",
        "# Calculate 95% confidence intervals for each metric\n",
        "def calculate_confidence_interval(scores):\n",
        "    lower_bound = np.percentile(scores, 2.5)\n",
        "    upper_bound = np.percentile(scores, 97.5)\n",
        "    return lower_bound, upper_bound\n",
        "\n",
        "# Calculate and print 95% confidence intervals\n",
        "accuracy_ci = calculate_confidence_interval(accuracy_scores)\n",
        "precision_ci = calculate_confidence_interval(precision_scores)\n",
        "recall_ci = calculate_confidence_interval(recall_scores)\n",
        "f1_ci = calculate_confidence_interval(f1_scores)\n",
        "roc_auc_ci = calculate_confidence_interval(roc_auc_scores)\n",
        "specificity_ci = calculate_confidence_interval(specificity_scores)\n",
        "\n",
        "print(f'Validation ROC AUC: {roc_auc_val:.2f}, 95% CI: [{roc_auc_ci[0]:.2f}, {roc_auc_ci[1]:.2f}]')\n",
        "print(f'Validation Accuracy: {accuracy_val:.2f}, 95% CI: [{accuracy_ci[0]:.2f}, {accuracy_ci[1]:.2f}]')\n",
        "print(f'Validation Precision: {precision_val:.2f}, 95% CI: [{precision_ci[0]:.2f}, {precision_ci[1]:.2f}]')\n",
        "print(f'Validation Recall: {recall_val:.2f}, 95% CI: [{recall_ci[0]:.2f}, {recall_ci[1]:.2f}]')\n",
        "print(f'Validation F1-Score: {f1_val:.2f}, 95% CI: [{f1_ci[0]:.2f}, {f1_ci[1]:.2f}]')\n",
        "print(f'Validation Specificity: {specificity_val:.2f}, 95% CI: [{specificity_ci[0]:.2f}, {specificity_ci[1]:.2f}]')\n"
      ],
      "metadata": {
        "id": "7-l7gxojnU8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LASSO"
      ],
      "metadata": {
        "id": "MbjXhvAmXP17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = gene.drop(['Group'], axis=1)\n",
        "y = gene['Group']\n",
        "\n",
        "# Encode categorical target labels into numerical labels\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split data into training and testing sets (25% test, 75% train)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Create Logistic Regression classifier with L1 regularization (Lasso)\n",
        "log_reg = LogisticRegression(penalty='l1', solver='liblinear', random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = log_reg.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average=\"weighted\", zero_division=1)\n",
        "recall = recall_score(y_test, y_pred, average=\"weighted\", zero_division=1)\n",
        "f1 = f1_score(y_test, y_pred, average=\"weighted\", zero_division=1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'F1 Score: {f1:.4f}')"
      ],
      "metadata": {
        "id": "J8CdDmaYXSSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Random Search"
      ],
      "metadata": {
        "id": "U-kT41pcbQcc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Define the parameter grid for Randomized Search\n",
        "param_dist = {\n",
        "    'C': np.logspace(-4, 4, 20),\n",
        "    'solver': ['liblinear'],\n",
        "    'max_iter': [1000, 5000, 10000, 20000],\n",
        "    'tol': [1e-4, 1e-3, 1e-2, 1e-1]\n",
        "}\n",
        "\n",
        "# Create Logistic Regression classifier with L1 regularization (Lasso)\n",
        "log_reg = LogisticRegression(penalty='l1', random_state=42)\n",
        "\n",
        "# Set up the Randomized Search with cross-validation\n",
        "random_search = RandomizedSearchCV(\n",
        "    log_reg, param_distributions=param_dist, n_iter=100,\n",
        "    scoring='roc_auc', cv=5, verbose=1, random_state=42, n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit the Randomized Search model\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "best_params = random_search.best_params_\n",
        "print(\"Best Parameters:\", best_params)\n",
        "\n",
        "# Predict on the test set with the best model\n",
        "y_pred = random_search.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted',zero_division=1)\n",
        "recall = recall_score(y_test, y_pred, average='weighted',zero_division=1)\n",
        "f1 = f1_score(y_test, y_pred, average='weighted', zero_division=1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'F1 Score: {f1:.4f}')\n"
      ],
      "metadata": {
        "id": "GvZyktApAjoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bayesian Optimization"
      ],
      "metadata": {
        "id": "BWaXjZW0btxm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_search_params = {'tol': 0.1, 'solver': 'liblinear', 'max_iter': 5000, 'C': 1.623776739188721}\n",
        "\n",
        "def objective(trial):\n",
        "    C = trial.suggest_float('C', 1e-4, 2000.0, log=True)\n",
        "    max_iter = trial.suggest_int('max_iter', 1000, 500000)\n",
        "    tol = trial.suggest_float('tol', 1e-5, 1e2, log=True)\n",
        "    solver = trial.suggest_categorical('solver', ['liblinear', 'saga'])\n",
        "        clf = LogisticRegression(\n",
        "            penalty='l1', C=C, max_iter=max_iter, tol=tol, solver=solver, random_state=42\n",
        "        )\n",
        "        score = cross_val_score(clf, X_train, y_train, cv=5, scoring='roc_auc').mean()\n",
        "\n",
        "    return score\n",
        "\n",
        "# Set up Optuna study\n",
        "study = optuna.create_study(direction='maximize')\n",
        "\n",
        "# Enqueue initial trial with prior parameters\n",
        "study.enqueue_trial(random_search_params)\n",
        "\n",
        "# Run optimization\n",
        "study.optimize(objective, n_trials=50)\n",
        "\n",
        "# Retrieve the best parameters and score\n",
        "print(\"Best Parameters from Optuna:\", study.best_params)\n",
        "print(\"Best AUC-ROC Score from Optuna:\", study.best_value)\n",
        "\n",
        "# Train the best model\n",
        "best_params = study.best_params\n",
        "clf = LogisticRegression(\n",
        "    penalty='l1', **best_params, random_state=42\n",
        ")\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_prob = clf.predict_proba(X_test)[:, 1]\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluation metrics on test set\n",
        "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted',zero_division=1)\n",
        "recall = recall_score(y_test, y_pred, average='weighted',zero_division=1)\n",
        "f1 = f1_score(y_test, y_pred, average='weighted',zero_division=1)\n",
        "\n",
        "# Print evaluation results\n",
        "print(f\"Test AUC-ROC: {roc_auc:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Test Precision: {precision:.4f}\")\n",
        "print(f\"Test Recall: {recall:.4f}\")\n",
        "print(f\"Test F1 Score: {f1:.4f}\")"
      ],
      "metadata": {
        "id": "A9DpV9ARCqEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIXT8hCDr8GW"
      },
      "outputs": [],
      "source": [
        "selected_features_lasso = X_train.columns.tolist()\n",
        "\n",
        "# Best parameters from Bayesian Optimization with Optuna\n",
        "best_params_lasso = {\n",
        "    'C':1.623776739188721,\n",
        "    'max_iter': 5000,\n",
        "    'solver': 'liblinear',\n",
        "    'tol':0.1\n",
        "}\n",
        "\n",
        "# Train the Logistic Regression model again using only the selected features\n",
        "final_model_lasso = LogisticRegression(penalty='l1', **best_params_lasso, random_state=42)\n",
        "\n",
        "# Perform cross-validation with 5 folds\n",
        "cv_scores = cross_val_score(final_model_lasso, X_train[selected_features_lasso], y_train, cv=5, scoring='roc_auc')\n",
        "print(\"Cross-validation scores:\")\n",
        "print(cv_scores)\n",
        "print(f\"Mean CV accuracy: {np.mean(cv_scores):.4f}\")\n",
        "\n",
        "final_model_lasso.fit(X_train[selected_features_lasso], y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_proba_lasso = final_model_lasso.predict_proba(X_test[selected_features_lasso])[:, 1]\n",
        "\n",
        "# Convert probabilities to predicted class labels\n",
        "y_pred_lasso = final_model_lasso.predict(X_test[selected_features_lasso])\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred_lasso)\n",
        "precision = precision_score(y_test, y_pred_lasso, average = 'weighted',zero_division=1)\n",
        "recall = recall_score(y_test, y_pred_lasso, average = 'weighted',zero_division=1)\n",
        "f1 = f1_score(y_test, y_pred_lasso, average = 'weighted',zero_division=1)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba_lasso)\n",
        "\n",
        "# Calculate confusion matrix and extract TN, FP, FN, TP\n",
        "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_lasso).ravel()\n",
        "\n",
        "# Calculate specificity\n",
        "specificity = tn / (tn + fp)\n",
        "\n",
        "print(f'Test ROC AUC: {roc_auc:.2f}')\n",
        "print(f'Test Accuracy: {accuracy:.2f}')\n",
        "print(f'Test Precision: {precision:.2f}')\n",
        "print(f'Test Recall: {recall:.2f}')\n",
        "print(f'Test F1-Score: {f1:.2f}')\n",
        "print(f'Test Specificity: {specificity:.2f}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##LASSO Validation"
      ],
      "metadata": {
        "id": "P4HntYRFcMIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation set\n",
        "X_val_cc_lasso = gene_cc.drop('Group', axis=1)\n",
        "y_val_cc_lasso = gene_cc['Group']\n",
        "\n",
        "# Encode categorical target labels into numerical labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y_val_cc_lasso)\n",
        "\n",
        "# Identify the features that are missing in the validation set\n",
        "missing_features = [feature for feature in selected_features_lasso if feature not in X_val_cc_lasso.columns]\n",
        "\n",
        "# Add the missing features to the validation set with zero values using pd.concat\n",
        "missing_df_lasso = pd.DataFrame(0, index=X_val_cc_lasso.index, columns=missing_features)\n",
        "X_val_cc_lasso = pd.concat([X_val_cc_lasso, missing_df_lasso], axis=1)\n",
        "\n",
        "# Ensure the columns are in the same order as the training features\n",
        "X_val_cc_lasso = X_val_cc_lasso[selected_features_lasso]\n",
        "\n",
        "#Make predictions on the validation set\n",
        "y_pred_prob_lasso_cc= final_model_lasso.predict_proba(X_val_cc_lasso)[:, 1]\n",
        "y_pred_lasso_cc = final_model_lasso.predict(X_val_cc_lasso)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy_val = accuracy_score(y_encoded, y_pred_lasso_cc)\n",
        "precision_val = precision_score(y_encoded, y_pred_lasso_cc,zero_division=1)\n",
        "recall_val = recall_score(y_encoded, y_pred_lasso_cc, average=\"weighted\",zero_division=1)\n",
        "f1_val = f1_score(y_encoded, y_pred_lasso_cc,  average=\"weighted\",zero_division=1)\n",
        "roc_auc_val = roc_auc_score(y_encoded, y_pred_prob_lasso_cc)\n",
        "\n",
        "# Calculate specificity\n",
        "tn_val, fp_val, fn_val, tp_val = confusion_matrix(y_encoded, y_pred_lasso_cc).ravel()\n",
        "specificity_val = tn_val / (tn_val + fp_val)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(f'Validation ROC AUC: {roc_auc_val:.2f}')\n",
        "print(f'Validation Accuracy: {accuracy_val:.2f}')\n",
        "print(f'Validation Precision: {precision_val:.2f}')\n",
        "print(f'Validation Recall: {recall_val:.2f}')\n",
        "print(f'Validation F1-Score: {f1_val:.2f}')\n",
        "print(f'Validation Specificity: {specificity_val:.2f}')"
      ],
      "metadata": {
        "id": "zcLwBdU1buyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#95% CI"
      ],
      "metadata": {
        "id": "qleuTjUccbo-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute metrics\n",
        "def compute_metrics(y_true, y_pred, y_pred_proba):\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred, average=\"weighted\")\n",
        "    f1 = f1_score(y_true, y_pred, average=\"weighted\")\n",
        "    roc_auc = roc_auc_score(y_true, y_pred_proba)\n",
        "\n",
        "    # Confusion matrix to compute specificity\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    specificity = tn / (tn + fp)\n",
        "\n",
        "    return accuracy, precision, recall, f1, roc_auc, specificity\n",
        "\n",
        "# Number of bootstrap iterations\n",
        "n_iterations = 1000\n",
        "n_size = len(X_val_cc_lasso)\n",
        "\n",
        "# Initialize lists to store metric values for each bootstrap sample\n",
        "accuracy_scores = []\n",
        "precision_scores = []\n",
        "recall_scores = []\n",
        "f1_scores = []\n",
        "roc_auc_scores = []\n",
        "specificity_scores = []\n",
        "\n",
        "# Bootstrap procedure\n",
        "for i in range(n_iterations):\n",
        "    # Resample the validation set with replacement\n",
        "    X_resample, y_resample = resample(X_val_cc_lasso, y_encoded, n_samples=n_size, random_state=i)\n",
        "\n",
        "    # Make predictions on the resampled data\n",
        "    y_pred_resample = final_model_lasso.predict(X_resample)\n",
        "    y_pred_proba_resample = final_model_lasso.predict_proba(X_resample)[:, 1]\n",
        "\n",
        "    # Calculate metrics for this bootstrap sample\n",
        "    accuracy, precision, recall, f1, roc_auc, specificity = compute_metrics(y_resample, y_pred_resample, y_pred_proba_resample)\n",
        "\n",
        "    # Store the metrics\n",
        "    accuracy_scores.append(accuracy)\n",
        "    precision_scores.append(precision)\n",
        "    recall_scores.append(recall)\n",
        "    f1_scores.append(f1)\n",
        "    roc_auc_scores.append(roc_auc)\n",
        "    specificity_scores.append(specificity)\n",
        "\n",
        "# Calculate 95% confidence intervals for each metric\n",
        "def calculate_confidence_interval(scores):\n",
        "    lower_bound = np.percentile(scores, 2.5)\n",
        "    upper_bound = np.percentile(scores, 97.5)\n",
        "    return lower_bound, upper_bound\n",
        "\n",
        "# Print 95% confidence intervals\n",
        "accuracy_ci = calculate_confidence_interval(accuracy_scores)\n",
        "precision_ci = calculate_confidence_interval(precision_scores)\n",
        "recall_ci = calculate_confidence_interval(recall_scores)\n",
        "f1_ci = calculate_confidence_interval(f1_scores)\n",
        "roc_auc_ci = calculate_confidence_interval(roc_auc_scores)\n",
        "specificity_ci = calculate_confidence_interval(specificity_scores)\n",
        "\n",
        "print(f'Validation ROC AUC: {roc_auc_val:.2f}, 95% CI: [{roc_auc_ci[0]:.2f}, {roc_auc_ci[1]:.2f}]')\n",
        "print(f'Validation Accuracy: {accuracy_val:.2f}, 95% CI: [{accuracy_ci[0]:.2f}, {accuracy_ci[1]:.2f}]')\n",
        "print(f'Validation Precision: {precision_val:.2f}, 95% CI: [{precision_ci[0]:.2f}, {precision_ci[1]:.2f}]')\n",
        "print(f'Validation Recall: {recall_val:.2f}, 95% CI: [{recall_ci[0]:.2f}, {recall_ci[1]:.2f}]')\n",
        "print(f'Validation F1-Score: {f1_val:.2f}, 95% CI: [{f1_ci[0]:.2f}, {f1_ci[1]:.2f}]')\n",
        "print(f'Validation Specificity: {specificity_val:.2f}, 95% CI: [{specificity_ci[0]:.2f}, {specificity_ci[1]:.2f}]')\n"
      ],
      "metadata": {
        "id": "eMnUpMWpccxl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}